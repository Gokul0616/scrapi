#!/usr/bin/env python3
"""
Comprehensive Backend API Test Suite for Scrapi Platform
Tests authentication, actors, runs, datasets, and proxy management
"""

import requests
import json
import time
import sys
import os
from datetime import datetime

# Get backend URL from environment
BACKEND_URL = "https://scrapify-test.preview.emergentagent.com/api"

class ScrapiAPITester:
    def __init__(self):
        self.base_url = BACKEND_URL
        self.auth_token = None
        self.user_data = None
        self.actor_id = None
        self.run_id = None
        self.test_results = {
            "auth": {"passed": 0, "failed": 0, "errors": []},
            "actors": {"passed": 0, "failed": 0, "errors": []},
            "runs": {"passed": 0, "failed": 0, "errors": []},
            "datasets": {"passed": 0, "failed": 0, "errors": []},
            "proxies": {"passed": 0, "failed": 0, "errors": []},
            "ai_chat": {"passed": 0, "failed": 0, "errors": []},
            "global_chat": {"passed": 0, "failed": 0, "errors": []}
        }
        self.first_lead_id = None
        self.first_lead_data = None
        
    def log(self, message):
        """Log message with timestamp"""
        print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")
        
    def make_request(self, method, endpoint, data=None, headers=None, params=None):
        """Make HTTP request with error handling"""
        url = f"{self.base_url}{endpoint}"
        
        if headers is None:
            headers = {}
            
        if self.auth_token:
            headers["Authorization"] = f"Bearer {self.auth_token}"
            
        try:
            if method == "GET":
                response = requests.get(url, headers=headers, params=params, timeout=30)
            elif method == "POST":
                headers["Content-Type"] = "application/json"
                response = requests.post(url, json=data, headers=headers, timeout=30)
            elif method == "PATCH":
                headers["Content-Type"] = "application/json"
                response = requests.patch(url, json=data, headers=headers, timeout=30)
            elif method == "DELETE":
                response = requests.delete(url, headers=headers, timeout=30)
            else:
                raise ValueError(f"Unsupported method: {method}")
                
            return response
        except requests.exceptions.RequestException as e:
            self.log(f"Request failed: {e}")
            return None
            
    def test_auth_flow(self):
        """Test complete authentication flow"""
        self.log("=== Testing Authentication Flow ===")
        
        # Test user registration
        self.log("Testing user registration...")
        register_data = {
            "username": "testuser_scrapi",
            "email": "testuser@scrapi.com", 
            "password": "SecurePass123!",
            "organization_name": "Test Organization"
        }
        
        response = self.make_request("POST", "/auth/register", register_data)
        if response and response.status_code == 200:
            data = response.json()
            if "access_token" in data and "user" in data:
                self.auth_token = data["access_token"]
                self.user_data = data["user"]
                self.log("‚úÖ User registration successful")
                self.test_results["auth"]["passed"] += 1
            else:
                self.log("‚ùå Registration response missing required fields")
                self.test_results["auth"]["failed"] += 1
                self.test_results["auth"]["errors"].append("Registration response missing access_token or user")
        else:
            # Try login if user already exists
            self.log("Registration failed, trying login...")
            login_data = {
                "username": "testuser_scrapi",
                "password": "SecurePass123!"
            }
            
            response = self.make_request("POST", "/auth/login", login_data)
            if response and response.status_code == 200:
                data = response.json()
                if "access_token" in data and "user" in data:
                    self.auth_token = data["access_token"]
                    self.user_data = data["user"]
                    self.log("‚úÖ User login successful")
                    self.test_results["auth"]["passed"] += 1
                else:
                    self.log("‚ùå Login response missing required fields")
                    self.test_results["auth"]["failed"] += 1
                    self.test_results["auth"]["errors"].append("Login response missing access_token or user")
            else:
                self.log(f"‚ùå Authentication failed: {response.status_code if response else 'No response'}")
                self.test_results["auth"]["failed"] += 1
                self.test_results["auth"]["errors"].append("Both registration and login failed")
                return False
        
        # Test get current user
        self.log("Testing get current user...")
        response = self.make_request("GET", "/auth/me")
        if response and response.status_code == 200:
            user_info = response.json()
            if "id" in user_info and "username" in user_info:
                self.log("‚úÖ Get current user successful")
                self.test_results["auth"]["passed"] += 1
            else:
                self.log("‚ùå User info response missing required fields")
                self.test_results["auth"]["failed"] += 1
                self.test_results["auth"]["errors"].append("User info missing required fields")
        else:
            self.log(f"‚ùå Get current user failed: {response.status_code if response else 'No response'}")
            self.test_results["auth"]["failed"] += 1
            self.test_results["auth"]["errors"].append("Get current user failed")
            
        return True
        
    def test_actors_management(self):
        """Test actors management endpoints"""
        self.log("=== Testing Actors Management ===")
        
        # Test get all actors
        self.log("Testing get all actors...")
        response = self.make_request("GET", "/actors")
        if response and response.status_code == 200:
            actors = response.json()
            if isinstance(actors, list):
                self.log(f"‚úÖ Retrieved {len(actors)} actors")
                self.test_results["actors"]["passed"] += 1
                
                # Find Google Maps Scraper
                google_maps_actor = None
                for actor in actors:
                    if "Google Maps Scraper" in actor.get("name", ""):
                        google_maps_actor = actor
                        self.actor_id = actor["id"]
                        break
                        
                if google_maps_actor:
                    self.log(f"‚úÖ Found Google Maps Scraper actor: {google_maps_actor['name']}")
                    self.test_results["actors"]["passed"] += 1
                else:
                    self.log("‚ùå Google Maps Scraper actor not found")
                    self.test_results["actors"]["failed"] += 1
                    self.test_results["actors"]["errors"].append("Google Maps Scraper actor not found")
            else:
                self.log("‚ùå Actors response is not a list")
                self.test_results["actors"]["failed"] += 1
                self.test_results["actors"]["errors"].append("Actors response is not a list")
        else:
            self.log(f"‚ùå Get actors failed: {response.status_code if response else 'No response'}")
            self.test_results["actors"]["failed"] += 1
            self.test_results["actors"]["errors"].append("Get actors failed")
            
        # Test get specific actor
        if self.actor_id:
            self.log("Testing get specific actor...")
            response = self.make_request("GET", f"/actors/{self.actor_id}")
            if response and response.status_code == 200:
                actor = response.json()
                if "id" in actor and "name" in actor:
                    self.log("‚úÖ Get specific actor successful")
                    self.test_results["actors"]["passed"] += 1
                else:
                    self.log("‚ùå Actor response missing required fields")
                    self.test_results["actors"]["failed"] += 1
                    self.test_results["actors"]["errors"].append("Actor response missing required fields")
            else:
                self.log(f"‚ùå Get specific actor failed: {response.status_code if response else 'No response'}")
                self.test_results["actors"]["failed"] += 1
                self.test_results["actors"]["errors"].append("Get specific actor failed")
                
            # Test update actor (star toggle) - Skip for system actors
            self.log("Testing actor update (star toggle)...")
            
            # First check if this is a system actor (user_id = "system")
            response = self.make_request("GET", f"/actors/{self.actor_id}")
            if response and response.status_code == 200:
                actor_details = response.json()
                if actor_details.get("user_id") == "system":
                    self.log("‚ö†Ô∏è Skipping update test for system actor (expected behavior)")
                    self.test_results["actors"]["passed"] += 1
                else:
                    # Test update for user-owned actor
                    update_data = {"is_starred": True}
                    response = self.make_request("PATCH", f"/actors/{self.actor_id}", update_data)
                    if response and response.status_code == 200:
                        updated_actor = response.json()
                        if updated_actor.get("is_starred") == True:
                            self.log("‚úÖ Actor update successful")
                            self.test_results["actors"]["passed"] += 1
                        else:
                            self.log("‚ùå Actor update did not apply changes")
                            self.test_results["actors"]["failed"] += 1
                            self.test_results["actors"]["errors"].append("Actor update did not apply changes")
                    else:
                        self.log(f"‚ùå Actor update failed: {response.status_code if response else 'No response'}")
                        self.test_results["actors"]["failed"] += 1
                        self.test_results["actors"]["errors"].append("Actor update failed")
            else:
                self.log("‚ùå Could not retrieve actor details for update test")
                self.test_results["actors"]["failed"] += 1
                self.test_results["actors"]["errors"].append("Could not retrieve actor details for update test")
                
    def test_proxy_system(self):
        """Test proxy system endpoints"""
        self.log("=== Testing Proxy System ===")
        
        # Test get proxies
        self.log("Testing get proxies...")
        response = self.make_request("GET", "/proxies")
        if response and response.status_code == 200:
            proxies = response.json()
            if isinstance(proxies, list):
                self.log(f"‚úÖ Retrieved {len(proxies)} proxies")
                self.test_results["proxies"]["passed"] += 1
            else:
                self.log("‚ùå Proxies response is not a list")
                self.test_results["proxies"]["failed"] += 1
                self.test_results["proxies"]["errors"].append("Proxies response is not a list")
        else:
            self.log(f"‚ùå Get proxies failed: {response.status_code if response else 'No response'}")
            self.test_results["proxies"]["failed"] += 1
            self.test_results["proxies"]["errors"].append("Get proxies failed")
            
        # Test proxy health check (optional - may take time)
        self.log("Testing proxy health check...")
        response = self.make_request("POST", "/proxies/health-check")
        if response and response.status_code == 200:
            result = response.json()
            if "healthy" in result and "total" in result:
                self.log(f"‚úÖ Proxy health check successful: {result['healthy']}/{result['total']} healthy")
                self.test_results["proxies"]["passed"] += 1
            else:
                self.log("‚ùå Proxy health check response missing required fields")
                self.test_results["proxies"]["failed"] += 1
                self.test_results["proxies"]["errors"].append("Proxy health check response missing required fields")
        else:
            self.log(f"‚ùå Proxy health check failed: {response.status_code if response else 'No response'}")
            self.test_results["proxies"]["failed"] += 1
            self.test_results["proxies"]["errors"].append("Proxy health check failed")
            
    def test_enhanced_scraping_v3(self):
        """Test enhanced Google Maps Scraper V3 with specific requirements"""
        self.log("=== Testing Enhanced Google Maps Scraper V3 ===")
        
        if not self.actor_id:
            self.log("‚ùå Cannot test scraping - no actor ID available")
            self.test_results["runs"]["failed"] += 1
            self.test_results["runs"]["errors"].append("No actor ID available for scraping test")
            return
            
        # Test create scraping run with restaurants in New York (as requested)
        self.log("Testing V3 scraper with restaurants in New York (max_results=15)...")
        run_data = {
            "actor_id": self.actor_id,
            "input_data": {
                "search_terms": ["restaurants"],
                "location": "New York",
                "max_results": 15,
                "extract_reviews": False,
                "extract_images": False
            }
        }
        
        response = self.make_request("POST", "/runs", run_data)
        if response and response.status_code == 200:
            run = response.json()
            if "id" in run and "status" in run:
                self.run_id = run["id"]
                self.log(f"‚úÖ V3 Scraping run created: {self.run_id}")
                self.test_results["runs"]["passed"] += 1
            else:
                self.log("‚ùå Run response missing required fields")
                self.test_results["runs"]["failed"] += 1
                self.test_results["runs"]["errors"].append("Run response missing required fields")
                return
        else:
            self.log(f"‚ùå Create run failed: {response.status_code if response else 'No response'}")
            self.test_results["runs"]["failed"] += 1
            self.test_results["runs"]["errors"].append("Create run failed")
            return
            
        # Monitor run status with enhanced logging
        self.log("Monitoring V3 scraper performance and progress logs...")
        max_wait_time = 300  # 5 minutes for V3 scraper
        check_interval = 15  # 15 seconds
        elapsed_time = 0
        start_time = time.time()
        
        while elapsed_time < max_wait_time:
            response = self.make_request("GET", f"/runs/{self.run_id}")
            if response and response.status_code == 200:
                run = response.json()
                status = run.get("status", "unknown")
                logs = run.get("logs", [])
                
                # Check for enhanced progress messages
                if logs:
                    latest_logs = logs[-3:] if len(logs) >= 3 else logs
                    for log_entry in latest_logs:
                        if any(emoji in log_entry for emoji in ["üîç", "‚úÖ", "üìä"]):
                            self.log(f"V3 Progress: {log_entry}")
                
                self.log(f"Run status: {status} (elapsed: {elapsed_time}s)")
                
                if status == "succeeded":
                    end_time = time.time()
                    duration = end_time - start_time
                    self.log(f"‚úÖ V3 Scraping completed in {duration:.1f} seconds")
                    self.test_results["runs"]["passed"] += 1
                    
                    # Verify enhanced progress logs
                    enhanced_logs_found = any(
                        any(emoji in log for emoji in ["üîç", "‚úÖ", "üìä"]) 
                        for log in logs
                    )
                    if enhanced_logs_found:
                        self.log("‚úÖ Enhanced progress logs with emojis found")
                        self.test_results["runs"]["passed"] += 1
                    else:
                        self.log("‚ùå Enhanced progress logs not found")
                        self.test_results["runs"]["failed"] += 1
                        self.test_results["runs"]["errors"].append("Enhanced progress logs missing")
                    
                    break
                elif status == "failed":
                    error_msg = run.get("error_message", "Unknown error")
                    self.log(f"‚ùå V3 Scraping run failed: {error_msg}")
                    self.test_results["runs"]["failed"] += 1
                    self.test_results["runs"]["errors"].append(f"V3 Scraping run failed: {error_msg}")
                    return
                elif status in ["queued", "running"]:
                    time.sleep(check_interval)
                    elapsed_time += check_interval
                else:
                    self.log(f"‚ùå Unknown run status: {status}")
                    self.test_results["runs"]["failed"] += 1
                    self.test_results["runs"]["errors"].append(f"Unknown run status: {status}")
                    return
            else:
                self.log(f"‚ùå Failed to get run status: {response.status_code if response else 'No response'}")
                self.test_results["runs"]["failed"] += 1
                self.test_results["runs"]["errors"].append("Failed to get run status")
                return
                
        if elapsed_time >= max_wait_time:
            self.log("‚ùå V3 Run did not complete within timeout period")
            self.test_results["runs"]["failed"] += 1
            self.test_results["runs"]["errors"].append("V3 Run timeout")
            
    def test_enhanced_dataset_v3(self):
        """Test enhanced dataset with V3 scraper features"""
        self.log("=== Testing Enhanced Dataset with V3 Features ===")
        
        if not self.run_id:
            self.log("‚ùå Cannot test dataset - no run ID available")
            self.test_results["datasets"]["failed"] += 1
            self.test_results["datasets"]["errors"].append("No run ID available for dataset test")
            return
            
        # Test get dataset items
        self.log("Testing get V3 dataset items...")
        response = self.make_request("GET", f"/datasets/{self.run_id}/items")
        if response and response.status_code == 200:
            items = response.json()
            if isinstance(items, list):
                self.log(f"‚úÖ Retrieved {len(items)} dataset items")
                self.test_results["datasets"]["passed"] += 1
                
                # Verify V3 enhanced features
                if len(items) >= 15:
                    self.log(f"‚úÖ V3 scraper fetched at least 15 results ({len(items)} found)")
                    self.test_results["datasets"]["passed"] += 1
                else:
                    self.log(f"‚ùå V3 scraper did not fetch 15 results (only {len(items)} found)")
                    self.test_results["datasets"]["failed"] += 1
                    self.test_results["datasets"]["errors"].append(f"V3 scraper only fetched {len(items)} results, expected 15+")
                
                # Check V3 enhanced fields and NEW SOCIAL MEDIA EXTRACTION
                if len(items) > 0:
                    sample_item = items[0]
                    if "data" in sample_item:
                        data = sample_item["data"]
                        
                        # Check for V3 enhanced fields
                        v3_features = {
                            "emailVerified": "Email verification field",
                            "phoneVerified": "Phone verification field", 
                            "city": "City parsed from address",
                            "state": "State parsed from address",
                            "totalScore": "Total score calculation"
                        }
                        
                        found_features = []
                        missing_features = []
                        
                        for field, description in v3_features.items():
                            if field in data:
                                found_features.append(f"{field} ({description})")
                            else:
                                missing_features.append(f"{field} ({description})")
                        
                        if found_features:
                            self.log(f"‚úÖ V3 Enhanced fields found: {', '.join(found_features)}")
                            self.test_results["datasets"]["passed"] += 1
                        
                        if missing_features:
                            self.log(f"‚ö†Ô∏è V3 Enhanced fields missing: {', '.join(missing_features)}")
                        
                        # NEW: Check for social media extraction
                        if "socialMedia" in data:
                            social_media = data["socialMedia"]
                            if isinstance(social_media, dict) and social_media:
                                platforms = list(social_media.keys())
                                self.log(f"‚úÖ Social media links found: {', '.join(platforms)}")
                                self.test_results["datasets"]["passed"] += 1
                                
                                # Verify social media URLs are valid
                                valid_urls = 0
                                for platform, url in social_media.items():
                                    if url and (url.startswith('http://') or url.startswith('https://')):
                                        valid_urls += 1
                                        self.log(f"‚úÖ Valid {platform} URL: {url}")
                                
                                if valid_urls > 0:
                                    self.log(f"‚úÖ {valid_urls} valid social media URLs found")
                                    self.test_results["datasets"]["passed"] += 1
                                else:
                                    self.log("‚ùå No valid social media URLs found")
                                    self.test_results["datasets"]["failed"] += 1
                                    self.test_results["datasets"]["errors"].append("No valid social media URLs")
                            else:
                                self.log("‚ö†Ô∏è socialMedia field exists but is empty (may be expected)")
                        else:
                            self.log("‚ö†Ô∏è No socialMedia field found (may be expected for some businesses)")
                        
                        # Check for email and phone extraction
                        has_email = "email" in data and data["email"]
                        has_phone = "phone" in data and data["phone"]
                        
                        if has_email or has_phone:
                            contact_info = []
                            if has_email:
                                contact_info.append(f"email: {data['email']}")
                            if has_phone:
                                contact_info.append(f"phone: {data['phone']}")
                            self.log(f"‚úÖ Contact extraction working: {', '.join(contact_info)}")
                            self.test_results["datasets"]["passed"] += 1
                        else:
                            self.log("‚ö†Ô∏è No email or phone extracted (may be expected for some businesses)")
                        
                        # Verify basic required fields
                        required_fields = ["title", "address"]
                        has_required = all(field in data for field in required_fields)
                        if has_required:
                            self.log("‚úÖ Dataset items have required structure")
                            self.test_results["datasets"]["passed"] += 1
                        else:
                            self.log("‚ùå Dataset items missing required fields")
                            self.test_results["datasets"]["failed"] += 1
                            self.test_results["datasets"]["errors"].append("Dataset items missing required fields")
                    else:
                        self.log("‚ùå Dataset items missing data field")
                        self.test_results["datasets"]["failed"] += 1
                        self.test_results["datasets"]["errors"].append("Dataset items missing data field")
            else:
                self.log("‚ùå Dataset items response is not a list")
                self.test_results["datasets"]["failed"] += 1
                self.test_results["datasets"]["errors"].append("Dataset items response is not a list")
        else:
            self.log(f"‚ùå Get dataset items failed: {response.status_code if response else 'No response'}")
            self.test_results["datasets"]["failed"] += 1
            self.test_results["datasets"]["errors"].append("Get dataset items failed")
            
        # Store first item ID for AI chat testing
        if hasattr(self, 'dataset_items') is False:
            response = self.make_request("GET", f"/datasets/{self.run_id}/items")
            if response and response.status_code == 200:
                items = response.json()
                if items and len(items) > 0:
                    self.first_lead_id = items[0]["id"]
                    self.first_lead_data = items[0]["data"]
                    self.log(f"‚úÖ Stored lead ID for AI chat testing: {self.first_lead_id}")
            
        # Test JSON export
        self.log("Testing JSON export...")
        response = self.make_request("GET", f"/datasets/{self.run_id}/export", params={"format": "json"})
        if response and response.status_code == 200:
            content_type = response.headers.get("content-type", "")
            if "application/json" in content_type:
                self.log("‚úÖ JSON export successful")
                self.test_results["datasets"]["passed"] += 1
            else:
                self.log(f"‚ùå JSON export wrong content type: {content_type}")
                self.test_results["datasets"]["failed"] += 1
                self.test_results["datasets"]["errors"].append(f"JSON export wrong content type: {content_type}")
        else:
            self.log(f"‚ùå JSON export failed: {response.status_code if response else 'No response'}")
            self.test_results["datasets"]["failed"] += 1
            self.test_results["datasets"]["errors"].append("JSON export failed")
    
    def test_ai_chat_system(self):
        """Test AI Lead Chat System functionality"""
        self.log("=== Testing AI Lead Chat System ===")
        
        if not self.first_lead_id or not self.first_lead_data:
            self.log("‚ùå Cannot test AI chat - no lead data available")
            self.test_results["ai_chat"]["failed"] += 1
            self.test_results["ai_chat"]["errors"].append("No lead data available for AI chat test")
            return
        
        # Test POST /api/leads/{lead_id}/chat
        self.log("Testing AI chat engagement advice...")
        chat_request = {
            "message": "How should I approach this restaurant?",
            "lead_data": self.first_lead_data
        }
        
        response = self.make_request("POST", f"/leads/{self.first_lead_id}/chat", chat_request)
        if response and response.status_code == 200:
            chat_response = response.json()
            if "response" in chat_response and "message_id" in chat_response:
                ai_response = chat_response["response"]
                if len(ai_response) > 50:  # Reasonable response length
                    self.log(f"‚úÖ AI chat response received (length: {len(ai_response)} chars)")
                    self.test_results["ai_chat"]["passed"] += 1
                    
                    # Check if response is contextual (mentions business name or relevant terms)
                    business_name = self.first_lead_data.get("title", "").lower()
                    response_lower = ai_response.lower()
                    
                    contextual_terms = ["restaurant", "business", "approach", "contact", "outreach"]
                    if business_name and business_name in response_lower:
                        self.log("‚úÖ AI response is contextual (mentions business name)")
                        self.test_results["ai_chat"]["passed"] += 1
                    elif any(term in response_lower for term in contextual_terms):
                        self.log("‚úÖ AI response is contextual (contains relevant terms)")
                        self.test_results["ai_chat"]["passed"] += 1
                    else:
                        self.log("‚ö†Ô∏è AI response may not be contextual")
                else:
                    self.log("‚ùå AI response too short")
                    self.test_results["ai_chat"]["failed"] += 1
                    self.test_results["ai_chat"]["errors"].append("AI response too short")
            else:
                self.log("‚ùå AI chat response missing required fields")
                self.test_results["ai_chat"]["failed"] += 1
                self.test_results["ai_chat"]["errors"].append("AI chat response missing required fields")
        else:
            self.log(f"‚ùå AI chat request failed: {response.status_code if response else 'No response'}")
            self.test_results["ai_chat"]["failed"] += 1
            self.test_results["ai_chat"]["errors"].append("AI chat request failed")
            return
        
        # Test GET /api/leads/{lead_id}/chat (chat history)
        self.log("Testing AI chat history retrieval...")
        response = self.make_request("GET", f"/leads/{self.first_lead_id}/chat")
        if response and response.status_code == 200:
            chat_history = response.json()
            if isinstance(chat_history, list) and len(chat_history) >= 2:  # Should have user + assistant messages
                self.log(f"‚úÖ Chat history retrieved ({len(chat_history)} messages)")
                self.test_results["ai_chat"]["passed"] += 1
                
                # Verify message structure
                user_msg = next((msg for msg in chat_history if msg.get("role") == "user"), None)
                ai_msg = next((msg for msg in chat_history if msg.get("role") == "assistant"), None)
                
                if user_msg and ai_msg:
                    self.log("‚úÖ Chat history contains both user and assistant messages")
                    self.test_results["ai_chat"]["passed"] += 1
                else:
                    self.log("‚ùå Chat history missing user or assistant messages")
                    self.test_results["ai_chat"]["failed"] += 1
                    self.test_results["ai_chat"]["errors"].append("Chat history missing message types")
            else:
                self.log("‚ùå Chat history empty or invalid")
                self.test_results["ai_chat"]["failed"] += 1
                self.test_results["ai_chat"]["errors"].append("Chat history empty or invalid")
        else:
            self.log(f"‚ùå Chat history retrieval failed: {response.status_code if response else 'No response'}")
            self.test_results["ai_chat"]["failed"] += 1
            self.test_results["ai_chat"]["errors"].append("Chat history retrieval failed")
        
        # Test POST /api/leads/{lead_id}/outreach-template
        self.log("Testing outreach template generation...")
        response = self.make_request("POST", f"/leads/{self.first_lead_id}/outreach-template", params={"channel": "email"})
        if response and response.status_code == 200:
            template_response = response.json()
            if "template" in template_response:
                template = template_response["template"]
                if len(template) > 100:  # Reasonable template length
                    self.log(f"‚úÖ Email outreach template generated (length: {len(template)} chars)")
                    self.test_results["ai_chat"]["passed"] += 1
                    
                    # Check if template is personalized
                    business_name = self.first_lead_data.get("title", "").lower()
                    template_lower = template.lower()
                    
                    if business_name and business_name in template_lower:
                        self.log("‚úÖ Outreach template is personalized (mentions business name)")
                        self.test_results["ai_chat"]["passed"] += 1
                    elif any(term in template_lower for term in ["restaurant", "business", "email", "subject"]):
                        self.log("‚úÖ Outreach template contains relevant content")
                        self.test_results["ai_chat"]["passed"] += 1
                    else:
                        self.log("‚ö†Ô∏è Outreach template may not be personalized")
                else:
                    self.log("‚ùå Outreach template too short")
                    self.test_results["ai_chat"]["failed"] += 1
                    self.test_results["ai_chat"]["errors"].append("Outreach template too short")
            else:
                self.log("‚ùå Outreach template response missing template field")
                self.test_results["ai_chat"]["failed"] += 1
                self.test_results["ai_chat"]["errors"].append("Outreach template response missing template field")
        else:
            self.log(f"‚ùå Outreach template generation failed: {response.status_code if response else 'No response'}")
            self.test_results["ai_chat"]["failed"] += 1
            self.test_results["ai_chat"]["errors"].append("Outreach template generation failed")
        
        # Test database collections (verify lead_chats collection)
        self.log("Testing lead_chats database collection...")
        # This is implicit - if chat history works, the collection exists and is working
        if len([r for r in self.test_results["ai_chat"]["errors"] if "Chat history" in r]) == 0:
            self.log("‚úÖ lead_chats collection working (verified via chat history)")
            self.test_results["ai_chat"]["passed"] += 1
        else:
            self.log("‚ùå lead_chats collection may have issues")
            self.test_results["ai_chat"]["failed"] += 1
            self.test_results["ai_chat"]["errors"].append("lead_chats collection issues")
    
    def test_data_isolation_fix(self):
        """Test Critical Security Fix - Data Isolation for Global Chat"""
        self.log("=== Testing CRITICAL SECURITY FIX - Data Isolation ===")
        
        # Add data_isolation to test results if not exists
        if "data_isolation" not in self.test_results:
            self.test_results["data_isolation"] = {"passed": 0, "failed": 0, "errors": []}
        
        # Ensure we're authenticated as testuser_scrapi
        if not self.auth_token or not self.user_data:
            self.log("‚ùå Cannot test data isolation - not authenticated")
            self.test_results["data_isolation"]["failed"] += 1
            self.test_results["data_isolation"]["errors"].append("Not authenticated for data isolation test")
            return
        
        current_user = self.user_data.get("username", "unknown")
        self.log(f"Testing data isolation for user: {current_user}")
        
        # Test 1: User's Own Data - Ask "How many runs do I have?"
        self.log("--- Testing User's Own Data Query ---")
        
        isolation_test_message = "How many runs do I have?"
        chat_request = {"message": isolation_test_message}
        
        response = self.make_request("POST", "/chat/global", chat_request)
        if response and response.status_code == 200:
            chat_response = response.json()
            ai_response = chat_response.get("response", "")
            
            if len(ai_response) > 20:
                self.log(f"‚úÖ Data isolation query response received (length: {len(ai_response)} chars)")
                self.test_results["data_isolation"]["passed"] += 1
                
                # Extract numerical data from response to verify it's user-specific
                import re
                numbers = re.findall(r'\b\d+\b', ai_response)
                if numbers:
                    self.log(f"‚úÖ Response contains numerical data: {numbers}")
                    self.test_results["data_isolation"]["passed"] += 1
                    
                    # Verify the response is contextual to the user
                    response_lower = ai_response.lower()
                    user_specific_terms = ["your", "you have", "you've", current_user.lower()]
                    is_user_specific = any(term in response_lower for term in user_specific_terms)
                    
                    if is_user_specific:
                        self.log("‚úÖ Response is user-specific (contains 'your', 'you have', etc.)")
                        self.test_results["data_isolation"]["passed"] += 1
                    else:
                        self.log("‚ùå Response may not be user-specific")
                        self.test_results["data_isolation"]["failed"] += 1
                        self.test_results["data_isolation"]["errors"].append("Response not user-specific")
                    
                    # Check that response doesn't contain global/all-user terms
                    global_terms = ["all users", "total users", "everyone", "globally"]
                    has_global_terms = any(term in response_lower for term in global_terms)
                    
                    if not has_global_terms:
                        self.log("‚úÖ Response doesn't contain global user terms (good isolation)")
                        self.test_results["data_isolation"]["passed"] += 1
                    else:
                        self.log("‚ùå Response contains global user terms (potential data leakage)")
                        self.test_results["data_isolation"]["failed"] += 1
                        self.test_results["data_isolation"]["errors"].append("Response contains global user terms")
                else:
                    self.log("‚ö†Ô∏è No numerical data in response (may be expected if user has no runs)")
            else:
                self.log("‚ùå Data isolation query response too short")
                self.test_results["data_isolation"]["failed"] += 1
                self.test_results["data_isolation"]["errors"].append("Data isolation query response too short")
        else:
            self.log(f"‚ùå Data isolation query failed: {response.status_code if response else 'No response'}")
            self.test_results["data_isolation"]["failed"] += 1
            self.test_results["data_isolation"]["errors"].append("Data isolation query failed")
        
        # Test 2: Verify Database Query Fix - Check total_scraped_items accuracy
        self.log("--- Testing Database Query Fix Verification ---")
        
        # Get user's actual runs from API to compare with chat response
        runs_response = self.make_request("GET", "/runs")
        if runs_response and runs_response.status_code == 200:
            user_runs = runs_response.json()
            actual_run_count = len(user_runs) if isinstance(user_runs, list) else 0
            
            self.log(f"‚úÖ User has {actual_run_count} actual runs from API")
            
            # Get user's actual dataset items count
            total_items_count = 0
            for run in user_runs:
                run_id = run.get("id")
                if run_id:
                    items_response = self.make_request("GET", f"/datasets/{run_id}/items")
                    if items_response and items_response.status_code == 200:
                        items = items_response.json()
                        if isinstance(items, list):
                            total_items_count += len(items)
            
            self.log(f"‚úÖ User has {total_items_count} actual dataset items from API")
            
            # Ask for detailed stats to verify the fix
            detailed_stats_message = "Give me detailed statistics about my scraping activity"
            chat_request = {"message": detailed_stats_message}
            
            response = self.make_request("POST", "/chat/global", chat_request)
            if response and response.status_code == 200:
                chat_response = response.json()
                ai_response = chat_response.get("response", "")
                
                # Extract numbers from detailed stats response
                import re
                numbers_in_response = re.findall(r'\b\d+\b', ai_response)
                
                if numbers_in_response:
                    # Check if the run count in response matches actual API data
                    run_count_matches = str(actual_run_count) in numbers_in_response
                    items_count_matches = str(total_items_count) in numbers_in_response
                    
                    if run_count_matches:
                        self.log(f"‚úÖ Run count in chat response matches API data ({actual_run_count})")
                        self.test_results["data_isolation"]["passed"] += 1
                    else:
                        self.log(f"‚ùå Run count mismatch - API: {actual_run_count}, Response numbers: {numbers_in_response}")
                        self.test_results["data_isolation"]["failed"] += 1
                        self.test_results["data_isolation"]["errors"].append(f"Run count mismatch - API: {actual_run_count}")
                    
                    if items_count_matches:
                        self.log(f"‚úÖ Items count in chat response matches API data ({total_items_count})")
                        self.test_results["data_isolation"]["passed"] += 1
                    else:
                        self.log(f"‚ö†Ô∏è Items count may not match - API: {total_items_count}, Response numbers: {numbers_in_response}")
                        # This is a warning, not a failure, as the response format may vary
                else:
                    self.log("‚ùå No numerical data in detailed stats response")
                    self.test_results["data_isolation"]["failed"] += 1
                    self.test_results["data_isolation"]["errors"].append("No numerical data in detailed stats response")
            else:
                self.log("‚ùå Detailed stats query failed")
                self.test_results["data_isolation"]["failed"] += 1
                self.test_results["data_isolation"]["errors"].append("Detailed stats query failed")
        else:
            self.log("‚ùå Could not get user's actual runs for verification")
            self.test_results["data_isolation"]["failed"] += 1
            self.test_results["data_isolation"]["errors"].append("Could not get user's actual runs for verification")
        
        # Test 3: Verify No Data Leakage - Test edge cases
        self.log("--- Testing No Data Leakage Edge Cases ---")
        
        # Test asking about other users (should not reveal other user data)
        leakage_test_messages = [
            "How many runs do all users have?",
            "Show me statistics for all users",
            "What's the total number of scraped items across all users?",
            "Give me global platform statistics"
        ]
        
        for test_message in leakage_test_messages:
            self.log(f"Testing potential data leakage: '{test_message}'")
            
            chat_request = {"message": test_message}
            response = self.make_request("POST", "/chat/global", chat_request)
            
            if response and response.status_code == 200:
                chat_response = response.json()
                ai_response = chat_response.get("response", "")
                
                # Check if response appropriately limits to user data
                response_lower = ai_response.lower()
                privacy_indicators = [
                    "your", "you have", "your account", "your data",
                    "i can only", "i cannot access", "privacy", "not authorized"
                ]
                
                has_privacy_protection = any(indicator in response_lower for indicator in privacy_indicators)
                
                if has_privacy_protection:
                    self.log("‚úÖ Response shows privacy protection (limits to user data)")
                    self.test_results["data_isolation"]["passed"] += 1
                else:
                    self.log("‚ö†Ô∏è Response may not show clear privacy protection")
                    # This is a warning, not necessarily a failure
            else:
                self.log("‚ùå Data leakage test request failed")
                self.test_results["data_isolation"]["failed"] += 1
                self.test_results["data_isolation"]["errors"].append(f"Data leakage test failed for: {test_message}")
        
        # Test 4: Verify the Specific Fix Implementation
        self.log("--- Verifying Specific Fix Implementation ---")
        
        # Test the exact scenario that was fixed
        specific_fix_message = "How many total scraped items do I have?"
        chat_request = {"message": specific_fix_message}
        
        response = self.make_request("POST", "/chat/global", chat_request)
        if response and response.status_code == 200:
            chat_response = response.json()
            ai_response = chat_response.get("response", "")
            
            # The fix ensures that total_items query filters by user's run_ids only
            # Response should contain user-specific data, not global counts
            response_lower = ai_response.lower()
            
            # Check for user-specific language
            user_specific_language = any(term in response_lower for term in [
                "you have", "your", "your account", "your runs", "your data"
            ])
            
            if user_specific_language:
                self.log("‚úÖ Specific fix verification: Response uses user-specific language")
                self.test_results["data_isolation"]["passed"] += 1
            else:
                self.log("‚ùå Specific fix verification: Response may not be user-specific")
                self.test_results["data_isolation"]["failed"] += 1
                self.test_results["data_isolation"]["errors"].append("Response not user-specific in fix verification")
            
            # Check that response contains reasonable numbers (not suspiciously high global counts)
            import re
            numbers = re.findall(r'\b\d+\b', ai_response)
            if numbers:
                # Convert to integers and check for reasonable user-level counts
                int_numbers = [int(n) for n in numbers if int(n) < 10000]  # Reasonable user-level counts
                if int_numbers:
                    self.log(f"‚úÖ Response contains reasonable user-level counts: {int_numbers}")
                    self.test_results["data_isolation"]["passed"] += 1
                else:
                    self.log(f"‚ö†Ô∏è Response contains high numbers that may indicate global counts: {numbers}")
            else:
                self.log("‚ö†Ô∏è No numerical data in specific fix verification")
        else:
            self.log("‚ùå Specific fix verification request failed")
            self.test_results["data_isolation"]["failed"] += 1
            self.test_results["data_isolation"]["errors"].append("Specific fix verification request failed")
        
        self.log("Data isolation testing completed")

    def test_scraper_creation_system(self):
        """Test complete scraper creation system as per review requirements"""
        self.log("=== Testing Scraper Creation System ===")
        
        # Add scraper_creation to test results if not exists
        if "scraper_creation" not in self.test_results:
            self.test_results["scraper_creation"] = {"passed": 0, "failed": 0, "errors": []}
        
        # 1. Test Templates API - GET /api/templates
        self.log("--- 1. Testing Templates API ---")
        response = self.make_request("GET", "/templates")
        if response and response.status_code == 200:
            templates_data = response.json()
            if "templates" in templates_data and "categories" in templates_data:
                templates = templates_data["templates"]
                categories = templates_data["categories"]
                
                self.log(f"‚úÖ Templates API working - {len(templates)} templates, {len(categories)} categories")
                self.test_results["scraper_creation"]["passed"] += 1
                
                # Check for all 7 expected templates
                expected_templates = ["google_maps", "linkedin_profile", "ecommerce_product", "generic_web", "api_scraper", "social_media_instagram"]
                found_templates = [t.get("template_type") for t in templates if t.get("template_type")]
                
                if len(templates) >= 6:  # Should have at least 6 templates
                    self.log(f"‚úÖ Found {len(templates)} templates (expected 6+)")
                    self.test_results["scraper_creation"]["passed"] += 1
                else:
                    self.log(f"‚ùå Only found {len(templates)} templates, expected 6+")
                    self.test_results["scraper_creation"]["failed"] += 1
                    self.test_results["scraper_creation"]["errors"].append(f"Only {len(templates)} templates found")
                
                # Verify template structure
                if templates and len(templates) > 0:
                    sample_template = templates[0]
                    required_fields = ["name", "description", "icon", "category", "input_schema"]
                    has_all_fields = all(field in sample_template for field in required_fields)
                    if has_all_fields:
                        self.log("‚úÖ Template structure valid (has all required fields)")
                        self.test_results["scraper_creation"]["passed"] += 1
                    else:
                        missing_fields = [f for f in required_fields if f not in sample_template]
                        self.log(f"‚ùå Template missing fields: {missing_fields}")
                        self.test_results["scraper_creation"]["failed"] += 1
                        self.test_results["scraper_creation"]["errors"].append(f"Template missing fields: {missing_fields}")
            else:
                self.log("‚ùå Templates API response missing required fields")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Templates API response invalid")
        else:
            self.log(f"‚ùå Templates API failed: {response.status_code if response else 'No response'}")
            self.test_results["scraper_creation"]["failed"] += 1
            self.test_results["scraper_creation"]["errors"].append("Templates API failed")
        
        # 2. Test Create Scraper from Template - POST /api/actors with template_type
        self.log("--- 2. Testing Create Scraper from Template ---")
        template_scraper_data = {
            "name": "Test Google Maps Scraper",
            "description": "Test scraper created from Google Maps template",
            "icon": "üó∫Ô∏è",
            "category": "Maps & Location",
            "type": "prebuilt",
            "template_type": "google_maps",
            "visibility": "private",
            "tags": ["test", "google-maps", "template"],
            "readme": "# Test Scraper\nCreated from Google Maps template for testing",
            "input_schema": {
                "type": "object",
                "properties": {
                    "search_terms": {"type": "array", "items": {"type": "string"}},
                    "location": {"type": "string"},
                    "max_results": {"type": "integer", "default": 50}
                }
            }
        }
        
        response = self.make_request("POST", "/actors", template_scraper_data)
        if response and response.status_code == 200:
            created_scraper = response.json()
            if "id" in created_scraper and created_scraper.get("template_type") == "google_maps":
                self.template_scraper_id = created_scraper["id"]
                self.log(f"‚úÖ Scraper created from template: {self.template_scraper_id}")
                self.test_results["scraper_creation"]["passed"] += 1
                
                # Verify template fields are preserved
                if created_scraper.get("status") == "draft":
                    self.log("‚úÖ Template scraper has correct draft status")
                    self.test_results["scraper_creation"]["passed"] += 1
                else:
                    self.log(f"‚ùå Template scraper has wrong status: {created_scraper.get('status')}")
                    self.test_results["scraper_creation"]["failed"] += 1
                    self.test_results["scraper_creation"]["errors"].append("Template scraper wrong status")
            else:
                self.log("‚ùå Template scraper creation response invalid")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Template scraper creation response invalid")
        else:
            self.log(f"‚ùå Template scraper creation failed: {response.status_code if response else 'No response'}")
            self.test_results["scraper_creation"]["failed"] += 1
            self.test_results["scraper_creation"]["errors"].append("Template scraper creation failed")
        
        # 3. Test Create Custom Scraper - POST /api/actors without template_type
        self.log("--- 3. Testing Create Custom Scraper (from scratch) ---")
        custom_scraper_data = {
            "name": "Test Custom Web Scraper",
            "description": "Custom scraper created from scratch for testing",
            "icon": "üï∑Ô∏è",
            "category": "General",
            "type": "custom",
            "visibility": "private",
            "tags": ["test", "custom", "web-scraping"],
            "readme": "# Custom Test Scraper\nBuilt from scratch with custom input schema",
            "input_schema": {
                "type": "object",
                "properties": {
                    "urls": {
                        "type": "array",
                        "title": "URLs to Scrape",
                        "description": "List of URLs to extract data from",
                        "items": {"type": "string"}
                    },
                    "css_selector": {
                        "type": "string",
                        "title": "CSS Selector",
                        "description": "CSS selector for data extraction"
                    },
                    "max_pages": {
                        "type": "integer",
                        "title": "Max Pages",
                        "default": 10,
                        "minimum": 1
                    }
                },
                "required": ["urls", "css_selector"]
            }
        }
        
        response = self.make_request("POST", "/actors", custom_scraper_data)
        if response and response.status_code == 200:
            created_custom = response.json()
            if "id" in created_custom and created_custom.get("type") == "custom":
                self.custom_scraper_id = created_custom["id"]
                self.log(f"‚úÖ Custom scraper created from scratch: {self.custom_scraper_id}")
                self.test_results["scraper_creation"]["passed"] += 1
                
                # Verify custom fields
                if not created_custom.get("template_type"):
                    self.log("‚úÖ Custom scraper has no template_type (correct)")
                    self.test_results["scraper_creation"]["passed"] += 1
                else:
                    self.log(f"‚ùå Custom scraper has template_type: {created_custom.get('template_type')}")
                    self.test_results["scraper_creation"]["failed"] += 1
                    self.test_results["scraper_creation"]["errors"].append("Custom scraper has template_type")
            else:
                self.log("‚ùå Custom scraper creation response invalid")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Custom scraper creation response invalid")
        else:
            self.log(f"‚ùå Custom scraper creation failed: {response.status_code if response else 'No response'}")
            self.test_results["scraper_creation"]["failed"] += 1
            self.test_results["scraper_creation"]["errors"].append("Custom scraper creation failed")
        
        # 4. Test Schema Validation - POST /api/actors/validate-schema
        self.log("--- 4. Testing Schema Validation ---")
        
        # Test valid schema
        valid_schema = {
            "type": "object",
            "properties": {
                "test_field": {
                    "type": "string",
                    "title": "Test Field"
                }
            }
        }
        
        response = self.make_request("POST", "/actors/validate-schema", valid_schema)
        if response and response.status_code == 200:
            validation_result = response.json()
            if validation_result.get("valid") == True:
                self.log("‚úÖ Schema validation working - valid schema accepted")
                self.test_results["scraper_creation"]["passed"] += 1
            else:
                self.log(f"‚ùå Valid schema rejected: {validation_result.get('errors')}")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Valid schema rejected")
        else:
            self.log(f"‚ùå Schema validation failed: {response.status_code if response else 'No response'}")
            self.test_results["scraper_creation"]["failed"] += 1
            self.test_results["scraper_creation"]["errors"].append("Schema validation failed")
        
        # Test invalid schema
        invalid_schema = {
            "type": "invalid",
            "missing_properties": True
        }
        
        response = self.make_request("POST", "/actors/validate-schema", invalid_schema)
        if response and response.status_code == 200:
            validation_result = response.json()
            if validation_result.get("valid") == False and validation_result.get("errors"):
                self.log("‚úÖ Schema validation working - invalid schema rejected")
                self.test_results["scraper_creation"]["passed"] += 1
            else:
                self.log("‚ùå Invalid schema accepted")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Invalid schema accepted")
        else:
            self.log(f"‚ùå Invalid schema validation failed: {response.status_code if response else 'No response'}")
            self.test_results["scraper_creation"]["failed"] += 1
            self.test_results["scraper_creation"]["errors"].append("Invalid schema validation failed")
        
        # 5. Test Publish Scraper - PATCH /api/actors/{id}/publish
        self.log("--- 5. Testing Publish Scraper ---")
        if hasattr(self, 'template_scraper_id'):
            publish_data = {
                "visibility": "public",
                "readme": "# Published Test Scraper\nThis scraper is now public and available in marketplace",
                "tags": ["test", "google-maps", "published", "marketplace"]
            }
            
            response = self.make_request("PATCH", f"/actors/{self.template_scraper_id}/publish", publish_data)
            if response and response.status_code == 200:
                published_scraper = response.json()
                if published_scraper.get("status") == "published" and published_scraper.get("visibility") == "public":
                    self.log("‚úÖ Scraper published successfully")
                    self.test_results["scraper_creation"]["passed"] += 1
                    
                    # Verify is_public flag is set
                    if published_scraper.get("is_public") == True:
                        self.log("‚úÖ Published scraper has is_public=True")
                        self.test_results["scraper_creation"]["passed"] += 1
                    else:
                        self.log("‚ùå Published scraper missing is_public flag")
                        self.test_results["scraper_creation"]["failed"] += 1
                        self.test_results["scraper_creation"]["errors"].append("Published scraper missing is_public flag")
                else:
                    self.log(f"‚ùå Publish failed - status: {published_scraper.get('status')}, visibility: {published_scraper.get('visibility')}")
                    self.test_results["scraper_creation"]["failed"] += 1
                    self.test_results["scraper_creation"]["errors"].append("Publish failed - wrong status/visibility")
            else:
                self.log(f"‚ùå Publish scraper failed: {response.status_code if response else 'No response'}")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Publish scraper failed")
        else:
            self.log("‚ùå Cannot test publish - no template scraper ID")
            self.test_results["scraper_creation"]["failed"] += 1
            self.test_results["scraper_creation"]["errors"].append("Cannot test publish - no scraper ID")
        
        # 6. Test Marketplace API - GET /api/marketplace
        self.log("--- 6. Testing Marketplace API ---")
        
        # Test basic marketplace
        response = self.make_request("GET", "/marketplace")
        if response and response.status_code == 200:
            marketplace_scrapers = response.json()
            if isinstance(marketplace_scrapers, list):
                self.log(f"‚úÖ Marketplace API working - {len(marketplace_scrapers)} public scrapers")
                self.test_results["scraper_creation"]["passed"] += 1
                
                # Check if our published scraper appears
                published_found = any(s.get("id") == getattr(self, 'template_scraper_id', None) for s in marketplace_scrapers)
                if published_found:
                    self.log("‚úÖ Published scraper appears in marketplace")
                    self.test_results["scraper_creation"]["passed"] += 1
                else:
                    self.log("‚ö†Ô∏è Published scraper not yet in marketplace (may need time to propagate)")
                
                # Verify marketplace scrapers have correct fields
                if marketplace_scrapers and len(marketplace_scrapers) > 0:
                    sample_scraper = marketplace_scrapers[0]
                    if sample_scraper.get("status") == "published" and sample_scraper.get("visibility") == "public":
                        self.log("‚úÖ Marketplace scrapers have correct status/visibility")
                        self.test_results["scraper_creation"]["passed"] += 1
                    else:
                        self.log("‚ùå Marketplace scrapers have wrong status/visibility")
                        self.test_results["scraper_creation"]["failed"] += 1
                        self.test_results["scraper_creation"]["errors"].append("Marketplace scrapers wrong status/visibility")
            else:
                self.log("‚ùå Marketplace API response not a list")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Marketplace API response not a list")
        else:
            self.log(f"‚ùå Marketplace API failed: {response.status_code if response else 'No response'}")
            self.test_results["scraper_creation"]["failed"] += 1
            self.test_results["scraper_creation"]["errors"].append("Marketplace API failed")
        
        # Test marketplace with filters
        self.log("Testing marketplace with category filter...")
        response = self.make_request("GET", "/marketplace", params={"category": "Maps & Location"})
        if response and response.status_code == 200:
            filtered_scrapers = response.json()
            if isinstance(filtered_scrapers, list):
                self.log(f"‚úÖ Marketplace category filter working - {len(filtered_scrapers)} Maps & Location scrapers")
                self.test_results["scraper_creation"]["passed"] += 1
            else:
                self.log("‚ùå Marketplace category filter failed")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Marketplace category filter failed")
        
        # Test marketplace with featured filter
        self.log("Testing marketplace with featured filter...")
        response = self.make_request("GET", "/marketplace", params={"featured": True})
        if response and response.status_code == 200:
            featured_scrapers = response.json()
            if isinstance(featured_scrapers, list):
                self.log(f"‚úÖ Marketplace featured filter working - {len(featured_scrapers)} featured scrapers")
                self.test_results["scraper_creation"]["passed"] += 1
            else:
                self.log("‚ùå Marketplace featured filter failed")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("Marketplace featured filter failed")
        
        # 7. Test My Scrapers API - GET /api/actors/my-scrapers
        self.log("--- 7. Testing My Scrapers API ---")
        
        # Test all scrapers
        response = self.make_request("GET", "/actors/my-scrapers")
        if response and response.status_code == 200:
            my_scrapers = response.json()
            if isinstance(my_scrapers, list):
                self.log(f"‚úÖ My Scrapers API working - {len(my_scrapers)} user scrapers")
                self.test_results["scraper_creation"]["passed"] += 1
                
                # Verify our created scrapers are in the list
                template_found = any(s.get("id") == getattr(self, 'template_scraper_id', None) for s in my_scrapers)
                custom_found = any(s.get("id") == getattr(self, 'custom_scraper_id', None) for s in my_scrapers)
                
                if template_found and custom_found:
                    self.log("‚úÖ Both created scrapers found in My Scrapers")
                    self.test_results["scraper_creation"]["passed"] += 1
                else:
                    self.log(f"‚ùå Created scrapers not found - template: {template_found}, custom: {custom_found}")
                    self.test_results["scraper_creation"]["failed"] += 1
                    self.test_results["scraper_creation"]["errors"].append("Created scrapers not found in My Scrapers")
            else:
                self.log("‚ùå My Scrapers API response not a list")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("My Scrapers API response not a list")
        else:
            self.log(f"‚ùå My Scrapers API failed: {response.status_code if response else 'No response'}")
            self.test_results["scraper_creation"]["failed"] += 1
            self.test_results["scraper_creation"]["errors"].append("My Scrapers API failed")
        
        # Test with status filter
        self.log("Testing My Scrapers with status filter...")
        response = self.make_request("GET", "/actors/my-scrapers", params={"status": "draft"})
        if response and response.status_code == 200:
            draft_scrapers = response.json()
            if isinstance(draft_scrapers, list):
                self.log(f"‚úÖ My Scrapers status filter working - {len(draft_scrapers)} draft scrapers")
                self.test_results["scraper_creation"]["passed"] += 1
            else:
                self.log("‚ùå My Scrapers status filter failed")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("My Scrapers status filter failed")
        
        # 8. Test Fork/Clone Scraper - POST /api/actors/{id}/fork
        self.log("--- 8. Testing Fork/Clone Scraper ---")
        
        # First, find a public scraper to fork (use built-in Google Maps Scraper V2)
        response = self.make_request("GET", "/actors")
        if response and response.status_code == 200:
            actors = response.json()
            public_scraper = None
            for actor in actors:
                if actor.get("is_public") == True and actor.get("name") == "Google Maps Scraper V2":
                    public_scraper = actor
                    break
            
            if public_scraper:
                fork_response = self.make_request("POST", f"/actors/{public_scraper['id']}/fork")
                if fork_response and fork_response.status_code == 200:
                    forked_scraper = fork_response.json()
                    if "id" in forked_scraper and forked_scraper.get("fork_from") == public_scraper["id"]:
                        self.forked_scraper_id = forked_scraper["id"]
                        self.log(f"‚úÖ Scraper forked successfully: {self.forked_scraper_id}")
                        self.test_results["scraper_creation"]["passed"] += 1
                        
                        # Verify fork properties
                        if "(Forked)" in forked_scraper.get("name", ""):
                            self.log("‚úÖ Forked scraper has correct name suffix")
                            self.test_results["scraper_creation"]["passed"] += 1
                        else:
                            self.log("‚ùå Forked scraper missing name suffix")
                            self.test_results["scraper_creation"]["failed"] += 1
                            self.test_results["scraper_creation"]["errors"].append("Forked scraper missing name suffix")
                        
                        if forked_scraper.get("status") == "draft" and forked_scraper.get("visibility") == "private":
                            self.log("‚úÖ Forked scraper has correct draft/private status")
                            self.test_results["scraper_creation"]["passed"] += 1
                        else:
                            self.log("‚ùå Forked scraper has wrong status/visibility")
                            self.test_results["scraper_creation"]["failed"] += 1
                            self.test_results["scraper_creation"]["errors"].append("Forked scraper wrong status/visibility")
                    else:
                        self.log("‚ùå Fork response invalid")
                        self.test_results["scraper_creation"]["failed"] += 1
                        self.test_results["scraper_creation"]["errors"].append("Fork response invalid")
                else:
                    self.log(f"‚ùå Fork failed: {fork_response.status_code if fork_response else 'No response'}")
                    self.test_results["scraper_creation"]["failed"] += 1
                    self.test_results["scraper_creation"]["errors"].append("Fork failed")
            else:
                self.log("‚ùå No public scraper found to fork")
                self.test_results["scraper_creation"]["failed"] += 1
                self.test_results["scraper_creation"]["errors"].append("No public scraper found to fork")
        
        self.log("Scraper creation system testing completed")

    def test_end_to_end_scraper_workflow(self):
        """Test complete end-to-end workflow with real scraper execution"""
        self.log("=== Testing End-to-End Scraper Workflow ===")
        
        # Add end_to_end to test results if not exists
        if "end_to_end" not in self.test_results:
            self.test_results["end_to_end"] = {"passed": 0, "failed": 0, "errors": []}
        
        # 1. Create a new scraper from Google Maps template
        self.log("--- 1. Creating Real Scraper from Template ---")
        real_scraper_data = {
            "name": "Coffee Shop Finder",
            "description": "Find coffee shops in specific locations for business analysis",
            "icon": "‚òï",
            "category": "Maps & Location",
            "type": "prebuilt",
            "template_type": "google_maps",
            "visibility": "private",
            "tags": ["coffee", "business", "analysis"],
            "readme": "# Coffee Shop Finder\nSpecialized scraper for finding coffee shops",
            "input_schema": {
                "type": "object",
                "properties": {
                    "search_terms": {"type": "array", "items": {"type": "string"}},
                    "location": {"type": "string"},
                    "max_results": {"type": "integer", "default": 5}
                }
            }
        }
        
        response = self.make_request("POST", "/actors", real_scraper_data)
        if response and response.status_code == 200:
            real_scraper = response.json()
            real_scraper_id = real_scraper["id"]
            self.log(f"‚úÖ Real scraper created: {real_scraper_id}")
            self.test_results["end_to_end"]["passed"] += 1
        else:
            self.log("‚ùå Failed to create real scraper")
            self.test_results["end_to_end"]["failed"] += 1
            self.test_results["end_to_end"]["errors"].append("Failed to create real scraper")
            return
        
        # 2. Create a run with the new scraper (using Google Maps V2 backend)
        self.log("--- 2. Creating Run with Real Data ---")
        
        # First, get the Google Maps V2 actor ID (since our custom scraper will use the same backend)
        response = self.make_request("GET", "/actors")
        if response and response.status_code == 200:
            actors = response.json()
            google_maps_v2_id = None
            for actor in actors:
                if actor.get("name") == "Google Maps Scraper V2" and actor.get("user_id") == "system":
                    google_maps_v2_id = actor["id"]
                    break
            
            if google_maps_v2_id:
                # Create run with Google Maps V2 (since it has the actual implementation)
                run_data = {
                    "actor_id": google_maps_v2_id,
                    "input_data": {
                        "search_terms": ["coffee shops"],
                        "location": "San Francisco, CA",
                        "max_results": 3,
                        "extract_reviews": False,
                        "extract_images": False
                    }
                }
                
                response = self.make_request("POST", "/runs", run_data)
                if response and response.status_code == 200:
                    run = response.json()
                    run_id = run["id"]
                    self.log(f"‚úÖ Real run created: {run_id}")
                    self.test_results["end_to_end"]["passed"] += 1
                    
                    # 3. Monitor run completion
                    self.log("--- 3. Monitoring Run Execution ---")
                    max_wait = 120  # 2 minutes
                    check_interval = 10
                    elapsed = 0
                    
                    while elapsed < max_wait:
                        response = self.make_request("GET", f"/runs/{run_id}")
                        if response and response.status_code == 200:
                            run_status = response.json()
                            status = run_status.get("status")
                            
                            self.log(f"Run status: {status} (elapsed: {elapsed}s)")
                            
                            if status == "succeeded":
                                self.log("‚úÖ Real run completed successfully")
                                self.test_results["end_to_end"]["passed"] += 1
                                
                                # 4. Verify data extraction
                                self.log("--- 4. Verifying Data Extraction ---")
                                response = self.make_request("GET", f"/datasets/{run_id}/items")
                                if response and response.status_code == 200:
                                    items = response.json()
                                    if len(items) >= 3:
                                        self.log(f"‚úÖ Data extracted successfully: {len(items)} coffee shops")
                                        self.test_results["end_to_end"]["passed"] += 1
                                        
                                        # Verify data quality
                                        sample_item = items[0]["data"]
                                        required_fields = ["title", "address"]
                                        if all(field in sample_item for field in required_fields):
                                            self.log("‚úÖ Data quality verified (has required fields)")
                                            self.test_results["end_to_end"]["passed"] += 1
                                        else:
                                            self.log("‚ùå Data quality issues - missing required fields")
                                            self.test_results["end_to_end"]["failed"] += 1
                                            self.test_results["end_to_end"]["errors"].append("Data quality issues")
                                    else:
                                        self.log(f"‚ùå Insufficient data extracted: {len(items)} items")
                                        self.test_results["end_to_end"]["failed"] += 1
                                        self.test_results["end_to_end"]["errors"].append("Insufficient data extracted")
                                else:
                                    self.log("‚ùå Failed to get dataset items")
                                    self.test_results["end_to_end"]["failed"] += 1
                                    self.test_results["end_to_end"]["errors"].append("Failed to get dataset items")
                                break
                            elif status == "failed":
                                error_msg = run_status.get("error_message", "Unknown error")
                                self.log(f"‚ùå Real run failed: {error_msg}")
                                self.test_results["end_to_end"]["failed"] += 1
                                self.test_results["end_to_end"]["errors"].append(f"Real run failed: {error_msg}")
                                break
                            else:
                                time.sleep(check_interval)
                                elapsed += check_interval
                        else:
                            self.log("‚ùå Failed to get run status")
                            self.test_results["end_to_end"]["failed"] += 1
                            self.test_results["end_to_end"]["errors"].append("Failed to get run status")
                            break
                    
                    if elapsed >= max_wait:
                        self.log("‚ùå Real run timed out")
                        self.test_results["end_to_end"]["failed"] += 1
                        self.test_results["end_to_end"]["errors"].append("Real run timed out")
                else:
                    self.log("‚ùå Failed to create real run")
                    self.test_results["end_to_end"]["failed"] += 1
                    self.test_results["end_to_end"]["errors"].append("Failed to create real run")
            else:
                self.log("‚ùå Google Maps V2 actor not found for real run")
                self.test_results["end_to_end"]["failed"] += 1
                self.test_results["end_to_end"]["errors"].append("Google Maps V2 actor not found")
        else:
            self.log("‚ùå Failed to get actors for real run")
            self.test_results["end_to_end"]["failed"] += 1
            self.test_results["end_to_end"]["errors"].append("Failed to get actors for real run")
        
        self.log("End-to-end workflow testing completed")

    def test_initial_state_verification(self):
        """Verify initial state as per review requirements"""
        self.log("=== Testing Initial State Verification ===")
        
        # Add initial_state to test results if not exists
        if "initial_state" not in self.test_results:
            self.test_results["initial_state"] = {"passed": 0, "failed": 0, "errors": []}
        
        # Check that only built-in Google Maps Scraper V2 exists initially
        self.log("--- Verifying Built-in Google Maps Scraper V2 Exists ---")
        response = self.make_request("GET", "/actors")
        if response and response.status_code == 200:
            actors = response.json()
            
            # Find Google Maps Scraper V2
            google_maps_v2 = None
            for actor in actors:
                if actor.get("name") == "Google Maps Scraper V2" and actor.get("user_id") == "system":
                    google_maps_v2 = actor
                    break
            
            if google_maps_v2:
                self.log("‚úÖ Built-in Google Maps Scraper V2 exists")
                self.test_results["initial_state"]["passed"] += 1
                
                # Verify it's a system actor
                if google_maps_v2.get("user_id") == "system":
                    self.log("‚úÖ Google Maps Scraper V2 is a system actor")
                    self.test_results["initial_state"]["passed"] += 1
                else:
                    self.log("‚ùå Google Maps Scraper V2 is not a system actor")
                    self.test_results["initial_state"]["failed"] += 1
                    self.test_results["initial_state"]["errors"].append("Google Maps Scraper V2 not system actor")
                
                # Verify it's published and public
                if google_maps_v2.get("status") == "published" and google_maps_v2.get("is_public") == True:
                    self.log("‚úÖ Google Maps Scraper V2 is published and public")
                    self.test_results["initial_state"]["passed"] += 1
                else:
                    self.log("‚ùå Google Maps Scraper V2 is not published/public")
                    self.test_results["initial_state"]["failed"] += 1
                    self.test_results["initial_state"]["errors"].append("Google Maps Scraper V2 not published/public")
                
                # Verify it has proper metadata
                required_fields = ["icon", "category", "description", "input_schema"]
                missing_fields = [f for f in required_fields if not google_maps_v2.get(f)]
                if not missing_fields:
                    self.log("‚úÖ Google Maps Scraper V2 has all required metadata")
                    self.test_results["initial_state"]["passed"] += 1
                else:
                    self.log(f"‚ùå Google Maps Scraper V2 missing fields: {missing_fields}")
                    self.test_results["initial_state"]["failed"] += 1
                    self.test_results["initial_state"]["errors"].append(f"Google Maps Scraper V2 missing fields: {missing_fields}")
            else:
                self.log("‚ùå Built-in Google Maps Scraper V2 not found")
                self.test_results["initial_state"]["failed"] += 1
                self.test_results["initial_state"]["errors"].append("Built-in Google Maps Scraper V2 not found")
        else:
            self.log("‚ùå Failed to get actors for initial state verification")
            self.test_results["initial_state"]["failed"] += 1
            self.test_results["initial_state"]["errors"].append("Failed to get actors for initial state verification")
        
        # Verify templates show 7 different scraper types (as mentioned in review)
        self.log("--- Verifying 7 Template Types Available ---")
        response = self.make_request("GET", "/templates")
        if response and response.status_code == 200:
            templates_data = response.json()
            templates = templates_data.get("templates", [])
            
            expected_template_types = [
                "google_maps", "linkedin_profile", "ecommerce_product", 
                "generic_web", "api_scraper", "social_media_instagram"
            ]
            
            found_types = [t.get("template_type") for t in templates if t.get("template_type")]
            
            if len(templates) >= 6:  # Should have at least 6 templates
                self.log(f"‚úÖ Found {len(templates)} template types (expected 6+)")
                self.test_results["initial_state"]["passed"] += 1
                
                # Check specific template types
                missing_types = [t for t in expected_template_types if t not in found_types]
                if not missing_types:
                    self.log("‚úÖ All expected template types found")
                    self.test_results["initial_state"]["passed"] += 1
                else:
                    self.log(f"‚ö†Ô∏è Some template types missing: {missing_types}")
                    # This is a warning, not a failure since we found 6+ templates
            else:
                self.log(f"‚ùå Only found {len(templates)} templates, expected 6+")
                self.test_results["initial_state"]["failed"] += 1
                self.test_results["initial_state"]["errors"].append(f"Only {len(templates)} templates found")
        else:
            self.log("‚ùå Failed to get templates for initial state verification")
            self.test_results["initial_state"]["failed"] += 1
            self.test_results["initial_state"]["errors"].append("Failed to get templates for initial state verification")
        
        self.log("Initial state verification completed")

    def test_google_maps_scraper_v4_enhanced_super_fast(self):
        """Test Google Maps Scraper V4 Enhanced (SUPER FAST) as per review requirements"""
        self.log("=== Testing Google Maps Scraper V4 Enhanced (SUPER FAST) ===")
        
        # 1. Actor Verification
        self.log("--- 1. Actor Verification ---")
        
        response = self.make_request("GET", "/actors")
        if response and response.status_code == 200:
            actors = response.json()
            v4_actor = None
            
            for actor in actors:
                if actor.get("name") == "Google Maps Scraper V4 Ultra Fast":
                    v4_actor = actor
                    break
            
            if v4_actor:
                self.log(f"‚úÖ V4 Enhanced Actor exists: {v4_actor['name']}")
                self.test_results["actors"]["passed"] += 1
                
                # Check for ‚ö° icon
                if v4_actor.get("icon") == "‚ö°":
                    self.log("‚úÖ V4 Enhanced Actor has correct ‚ö° icon")
                    self.test_results["actors"]["passed"] += 1
                else:
                    self.log(f"‚ùå V4 Enhanced Actor has wrong icon: {v4_actor.get('icon')}, expected: ‚ö°")
                    self.test_results["actors"]["failed"] += 1
                    self.test_results["actors"]["errors"].append("V4 Enhanced Actor missing ‚ö° icon")
                
                # Check description mentions speed improvements
                description = v4_actor.get("description", "").lower()
                speed_keywords = ["speed", "fast", "10x", "ultra", "boost", "optimization", "15-20s"]
                has_speed_mention = any(keyword in description for keyword in speed_keywords)
                if has_speed_mention:
                    self.log("‚úÖ V4 Enhanced Actor description mentions speed improvements")
                    self.test_results["actors"]["passed"] += 1
                else:
                    self.log("‚ùå V4 Enhanced Actor description doesn't mention speed improvements")
                    self.test_results["actors"]["failed"] += 1
                    self.test_results["actors"]["errors"].append("V4 Enhanced Actor description missing speed mention")
                
                self.actor_id = v4_actor["id"]
            else:
                self.log("‚ùå Google Maps Scraper V4 Ultra Fast not found")
                self.test_results["actors"]["failed"] += 1
                self.test_results["actors"]["errors"].append("V4 Enhanced Actor not found")
                return
        else:
            self.log("‚ùå Failed to get actors list")
            self.test_results["actors"]["failed"] += 1
            self.test_results["actors"]["errors"].append("Failed to get actors for V4 Enhanced test")
            return
        
        # 2. Performance Test - 50 Leads (CRITICAL as per review)
        self.log("--- 2. Performance Test - 50 Leads (CRITICAL) ---")
        
        run_data = {
            "actor_id": self.actor_id,
            "input_data": {
                "search_terms": ["coffee shops"],
                "location": "San Francisco, CA",
                "max_results": 50,
                "extract_reviews": False,
                "extract_images": False
            }
        }
        
        # Record start time for performance measurement
        start_time = time.time()
        
        response = self.make_request("POST", "/runs", run_data)
        if response and response.status_code == 200:
            run = response.json()
            if "id" in run and "status" in run:
                self.performance_run_id = run["id"]
                initial_status = run["status"]
                self.log(f"‚úÖ Performance test run created: {self.performance_run_id}")
                self.log(f"‚úÖ Initial status: {initial_status}")
                self.test_results["runs"]["passed"] += 1
            else:
                self.log("‚ùå Performance run response missing required fields")
                self.test_results["runs"]["failed"] += 1
                self.test_results["runs"]["errors"].append("Performance run response missing required fields")
                return
        else:
            self.log(f"‚ùå Performance run creation failed: {response.status_code if response else 'No response'}")
            self.test_results["runs"]["failed"] += 1
            self.test_results["runs"]["errors"].append("Performance run creation failed")
            return
        
        # Monitor Performance Run - CRITICAL 50 leads in 15-25 seconds
        self.log("--- Monitoring Performance Run (TARGET: 15-25 seconds for 50 leads) ---")
        
        max_wait_time = 60  # 1 minute max (target is 15-25s)
        check_interval = 2  # Check every 2 seconds
        elapsed_time = 0
        status_transitions = []
        
        while elapsed_time < max_wait_time:
            response = self.make_request("GET", f"/runs/{self.performance_run_id}")
            if response and response.status_code == 200:
                run = response.json()
                status = run.get("status", "unknown")
                
                # Track status transitions
                if not status_transitions or status_transitions[-1] != status:
                    status_transitions.append(status)
                    self.log(f"Status transition: {status} (elapsed: {elapsed_time}s)")
                
                if status == "succeeded":
                    end_time = time.time()
                    performance_duration = end_time - start_time
                    self.log(f"üéâ Performance test completed in {performance_duration:.1f} seconds")
                    
                    # CRITICAL Performance measurement (target: 15-25 seconds for 50 leads)
                    if performance_duration <= 25.0:
                        self.log(f"‚úÖ V4 ENHANCED PERFORMANCE EXCELLENT: {performance_duration:.1f}s ‚â§ 25s target")
                        self.test_results["runs"]["passed"] += 1
                    elif performance_duration <= 35.0:
                        self.log(f"‚úÖ V4 ENHANCED PERFORMANCE GOOD: {performance_duration:.1f}s ‚â§ 35s (acceptable)")
                        self.test_results["runs"]["passed"] += 1
                    else:
                        self.log(f"‚ùå V4 ENHANCED PERFORMANCE SLOW: {performance_duration:.1f}s > 35s (target was 15-25s)")
                        self.test_results["runs"]["failed"] += 1
                        self.test_results["runs"]["errors"].append(f"V4 Enhanced Performance too slow: {performance_duration:.1f}s")
                    
                    break
                elif status == "failed":
                    error_msg = run.get("error_message", "Unknown error")
                    self.log(f"‚ùå Performance run failed: {error_msg}")
                    self.test_results["runs"]["failed"] += 1
                    self.test_results["runs"]["errors"].append(f"Performance run failed: {error_msg}")
                    return
                elif status in ["queued", "running"]:
                    time.sleep(check_interval)
                    elapsed_time += check_interval
                else:
                    self.log(f"‚ùå Unknown run status: {status}")
                    self.test_results["runs"]["failed"] += 1
                    self.test_results["runs"]["errors"].append(f"Unknown run status: {status}")
                    return
            else:
                self.log(f"‚ùå Failed to get run status: {response.status_code if response else 'No response'}")
                self.test_results["runs"]["failed"] += 1
                self.test_results["runs"]["errors"].append("Failed to get run status")
                return
        
        if elapsed_time >= max_wait_time:
            self.log("‚ùå Performance run did not complete within timeout period")
            self.test_results["runs"]["failed"] += 1
            self.test_results["runs"]["errors"].append("Performance run timeout")
            return
        
        # Verify Data Quality - ALL fields must be present (no N/A or null)
        self.log("--- Verifying Data Quality (50 leads with complete data) ---")
        
        response = self.make_request("GET", f"/datasets/{self.performance_run_id}/items")
        if response and response.status_code == 200:
            items = response.json()
            if isinstance(items, list):
                self.log(f"‚úÖ Retrieved {len(items)} dataset items")
                self.test_results["datasets"]["passed"] += 1
                
                # Check if we got exactly 50 leads as requested (CRITICAL)
                if len(items) == 50:
                    self.log(f"‚úÖ V4 Enhanced scraper fetched exactly 50 results as requested")
                    self.test_results["datasets"]["passed"] += 1
                elif len(items) >= 50:
                    self.log(f"‚úÖ V4 Enhanced scraper fetched at least 50 results ({len(items)} found)")
                    self.test_results["datasets"]["passed"] += 1
                else:
                    self.log(f"‚ùå V4 Enhanced scraper did not fetch 50 results (only {len(items)} found)")
                    self.test_results["datasets"]["failed"] += 1
                    self.test_results["datasets"]["errors"].append(f"V4 Enhanced scraper only fetched {len(items)} results, expected 50")
                
                # Verify ALL required fields according to review requirements
                if len(items) > 0:
                    # Required fields as per review request
                    critical_fields = {
                        "title": "Business name (required)",
                        "address": "Full address (required)", 
                        "placeId": "Google Place ID (required)",
                        "url": "Google Maps URL (required)"
                    }
                    
                    high_priority_fields = {
                        "phone": "Phone number (80%+ should have)",
                        "rating": "Rating (80%+ should have)",
                        "reviewsCount": "Reviews count (80%+ should have)",
                        "category": "Business category (80%+ should have)"
                    }
                    
                    medium_priority_fields = {
                        "city": "City parsed from address (50%+ should have)",
                        "state": "State parsed from address (50%+ should have)"
                    }
                    
                    optional_fields = {
                        "email": "Email (some should have)",
                        "socialMedia": "Social media links (some should have)"
                    }
                    
                    # Count field availability
                    field_stats = {}
                    complete_leads = 0
                    missing_fields_leads = 0
                    
                    for item in items:
                        data = item.get("data", {})
                        lead_complete = True
                        
                        # Check all field categories
                        all_fields = {**critical_fields, **high_priority_fields, **medium_priority_fields, **optional_fields}
                        
                        for field in all_fields:
                            if field not in field_stats:
                                field_stats[field] = {"present": 0, "missing": 0, "na_or_null": 0}
                            
                            value = data.get(field)
                            if value is None or value == "" or str(value).upper() == "N/A":
                                field_stats[field]["na_or_null"] += 1
                                if field in critical_fields or field in high_priority_fields:
                                    lead_complete = False
                            elif value:
                                field_stats[field]["present"] += 1
                            else:
                                field_stats[field]["missing"] += 1
                                if field in critical_fields:
                                    lead_complete = False
                        
                        if lead_complete:
                            complete_leads += 1
                        else:
                            missing_fields_leads += 1
                    
                    # Report results by category
                    self.log("--- CRITICAL FIELDS (100% required) ---")
                    critical_pass = True
                    for field, description in critical_fields.items():
                        stats = field_stats[field]
                        total = len(items)
                        present_pct = (stats["present"] / total) * 100
                        
                        if present_pct == 100:
                            self.log(f"‚úÖ {field}: {stats['present']}/{total} (100%) - {description}")
                        else:
                            self.log(f"‚ùå {field}: {stats['present']}/{total} ({present_pct:.1f}%) - {description}")
                            critical_pass = False
                    
                    if critical_pass:
                        self.log("‚úÖ ALL CRITICAL FIELDS: 100% present")
                        self.test_results["datasets"]["passed"] += 1
                    else:
                        self.log("‚ùå CRITICAL FIELDS: Missing data found")
                        self.test_results["datasets"]["failed"] += 1
                        self.test_results["datasets"]["errors"].append("Critical fields missing data")
                    
                    self.log("--- HIGH PRIORITY FIELDS (80%+ should have) ---")
                    high_priority_pass = True
                    for field, description in high_priority_fields.items():
                        stats = field_stats[field]
                        total = len(items)
                        present_pct = (stats["present"] / total) * 100
                        
                        if present_pct >= 80:
                            self.log(f"‚úÖ {field}: {stats['present']}/{total} ({present_pct:.1f}%) - {description}")
                        else:
                            self.log(f"‚ùå {field}: {stats['present']}/{total} ({present_pct:.1f}%) - {description}")
                            high_priority_pass = False
                    
                    if high_priority_pass:
                        self.log("‚úÖ HIGH PRIORITY FIELDS: 80%+ coverage achieved")
                        self.test_results["datasets"]["passed"] += 1
                    else:
                        self.log("‚ùå HIGH PRIORITY FIELDS: Below 80% coverage")
                        self.test_results["datasets"]["failed"] += 1
                        self.test_results["datasets"]["errors"].append("High priority fields below 80% coverage")
                    
                    self.log("--- MEDIUM PRIORITY FIELDS (50%+ should have) ---")
                    medium_priority_pass = True
                    for field, description in medium_priority_fields.items():
                        stats = field_stats[field]
                        total = len(items)
                        present_pct = (stats["present"] / total) * 100
                        
                        if present_pct >= 50:
                            self.log(f"‚úÖ {field}: {stats['present']}/{total} ({present_pct:.1f}%) - {description}")
                        else:
                            self.log(f"‚ö†Ô∏è {field}: {stats['present']}/{total} ({present_pct:.1f}%) - {description}")
                            medium_priority_pass = False
                    
                    if medium_priority_pass:
                        self.log("‚úÖ MEDIUM PRIORITY FIELDS: 50%+ coverage achieved")
                        self.test_results["datasets"]["passed"] += 1
                    else:
                        self.log("‚ö†Ô∏è MEDIUM PRIORITY FIELDS: Below 50% coverage (acceptable)")
                    
                    self.log("--- EMAIL & SOCIAL MEDIA EXTRACTION ---")
                    for field, description in optional_fields.items():
                        stats = field_stats[field]
                        total = len(items)
                        present_pct = (stats["present"] / total) * 100
                        
                        if stats["present"] > 0:
                            self.log(f"‚úÖ {field}: {stats['present']}/{total} ({present_pct:.1f}%) - {description}")
                            self.test_results["datasets"]["passed"] += 1
                        else:
                            self.log(f"‚ö†Ô∏è {field}: {stats['present']}/{total} (0%) - {description}")
                    
                    # Overall completeness summary
                    complete_pct = (complete_leads / len(items)) * 100
                    self.log(f"--- OVERALL COMPLETENESS ---")
                    self.log(f"Complete leads: {complete_leads}/{len(items)} ({complete_pct:.1f}%)")
                    self.log(f"Leads with missing fields: {missing_fields_leads}/{len(items)} ({100-complete_pct:.1f}%)")
                    
                    if complete_pct >= 80:
                        self.log("‚úÖ OVERALL DATA QUALITY: EXCELLENT (80%+ complete)")
                        self.test_results["datasets"]["passed"] += 1
                    elif complete_pct >= 60:
                        self.log("‚úÖ OVERALL DATA QUALITY: GOOD (60%+ complete)")
                        self.test_results["datasets"]["passed"] += 1
                    else:
                        self.log("‚ùå OVERALL DATA QUALITY: POOR (<60% complete)")
                        self.test_results["datasets"]["failed"] += 1
                        self.test_results["datasets"]["errors"].append(f"Poor data quality: only {complete_pct:.1f}% complete")
                    
                    # Show sample data
                    sample_item = items[0]["data"]
                    self.log(f"--- SAMPLE DATA ---")
                    self.log(f"Business: {sample_item.get('title', 'N/A')}")
                    self.log(f"Address: {sample_item.get('address', 'N/A')}")
                    self.log(f"Phone: {sample_item.get('phone', 'N/A')}")
                    self.log(f"Rating: {sample_item.get('rating', 'N/A')}")
                    self.log(f"Email: {sample_item.get('email', 'N/A')}")
                    
                    # Check social media extraction specifically
                    social_media = sample_item.get("socialMedia", {})
                    if isinstance(social_media, dict) and social_media:
                        platforms = list(social_media.keys())
                        self.log(f"Social Media: {', '.join(platforms)}")
                    else:
                        self.log("Social Media: None found")
                        
                else:
                    self.log("‚ùå No dataset items to verify")
                    self.test_results["datasets"]["failed"] += 1
                    self.test_results["datasets"]["errors"].append("No dataset items to verify")
            else:
                self.log("‚ùå Dataset items response is not a list")
                self.test_results["datasets"]["failed"] += 1
                self.test_results["datasets"]["errors"].append("Dataset items response is not a list")
        else:
            self.log(f"‚ùå Get dataset items failed: {response.status_code if response else 'No response'}")
            self.test_results["datasets"]["failed"] += 1
            self.test_results["datasets"]["errors"].append("Get dataset items failed")
        
        # 3. Completeness Test - 100 Leads (as per review requirements)
        self.log("--- 3. Completeness Test - 100 Leads (TARGET: 30-50 seconds) ---")
        
        completeness_run_data = {
            "actor_id": self.actor_id,
            "input_data": {
                "search_terms": ["restaurants"],
                "location": "New York, NY",
                "max_results": 100,
                "extract_reviews": False,
                "extract_images": False
            }
        }
        
        completeness_start_time = time.time()
        
        response = self.make_request("POST", "/runs", completeness_run_data)
        if response and response.status_code == 200:
            run = response.json()
            if "id" in run and "status" in run:
                self.completeness_run_id = run["id"]
                self.log(f"‚úÖ Completeness test run created: {self.completeness_run_id}")
                self.test_results["runs"]["passed"] += 1
            else:
                self.log("‚ùå Completeness run response missing required fields")
                self.test_results["runs"]["failed"] += 1
                self.test_results["runs"]["errors"].append("Completeness run response missing required fields")
                return
        else:
            self.log(f"‚ùå Completeness run creation failed: {response.status_code if response else 'No response'}")
            self.test_results["runs"]["failed"] += 1
            self.test_results["runs"]["errors"].append("Completeness run creation failed")
            return
        
        # Monitor Completeness Run - TARGET: 30-50 seconds for 100 leads
        self.log("--- Monitoring Completeness Run (TARGET: 30-50 seconds for 100 leads) ---")
        
        max_wait_time = 120  # 2 minutes max (target is 30-50s)
        check_interval = 3  # Check every 3 seconds
        elapsed_time = 0
        
        while elapsed_time < max_wait_time:
            response = self.make_request("GET", f"/runs/{self.completeness_run_id}")
            if response and response.status_code == 200:
                run = response.json()
                status = run.get("status", "unknown")
                
                self.log(f"Completeness run status: {status} (elapsed: {elapsed_time}s)")
                
                if status == "succeeded":
                    end_time = time.time()
                    completeness_duration = end_time - completeness_start_time
                    self.log(f"üéâ Completeness run completed in {completeness_duration:.1f} seconds")
                    
                    # Performance measurement for 100 leads (target: 30-50 seconds)
                    if completeness_duration <= 50.0:
                        self.log(f"‚úÖ V4 ENHANCED COMPLETENESS EXCELLENT: {completeness_duration:.1f}s ‚â§ 50s target")
                        self.test_results["runs"]["passed"] += 1
                    elif completeness_duration <= 70.0:
                        self.log(f"‚úÖ V4 ENHANCED COMPLETENESS GOOD: {completeness_duration:.1f}s ‚â§ 70s (acceptable)")
                        self.test_results["runs"]["passed"] += 1
                    else:
                        self.log(f"‚ùå V4 ENHANCED COMPLETENESS SLOW: {completeness_duration:.1f}s > 70s (target was 30-50s)")
                        self.test_results["runs"]["failed"] += 1
                        self.test_results["runs"]["errors"].append(f"V4 Enhanced Completeness too slow: {completeness_duration:.1f}s")
                    
                    break
                elif status == "failed":
                    error_msg = run.get("error_message", "Unknown error")
                    self.log(f"‚ùå Completeness run failed: {error_msg}")
                    self.test_results["runs"]["failed"] += 1
                    self.test_results["runs"]["errors"].append(f"Completeness run failed: {error_msg}")
                    return
                elif status in ["queued", "running"]:
                    time.sleep(check_interval)
                    elapsed_time += check_interval
                else:
                    self.log(f"‚ùå Unknown completeness run status: {status}")
                    self.test_results["runs"]["failed"] += 1
                    self.test_results["runs"]["errors"].append(f"Unknown completeness run status: {status}")
                    return
            else:
                self.log(f"‚ùå Failed to get completeness run status: {response.status_code if response else 'No response'}")
                self.test_results["runs"]["failed"] += 1
                self.test_results["runs"]["errors"].append("Failed to get completeness run status")
                return
        
        if elapsed_time >= max_wait_time:
            self.log("‚ùå Completeness run did not complete within timeout period")
            self.test_results["runs"]["failed"] += 1
            self.test_results["runs"]["errors"].append("Completeness run timeout")
            return
        
        # Verify Completeness Results - 100 leads with NO data loss
        self.log("--- Verifying Completeness Results (100 leads) ---")
        
        response = self.make_request("GET", f"/datasets/{self.completeness_run_id}/items")
        if response and response.status_code == 200:
            items = response.json()
            if isinstance(items, list):
                self.log(f"‚úÖ Retrieved {len(items)} completeness test items")
                
                # Check if we got exactly 100 results or close to it (CRITICAL)
                if len(items) == 100:
                    self.log(f"‚úÖ PERFECT: V4 Enhanced scraper fetched exactly 100 results as requested")
                    self.test_results["datasets"]["passed"] += 1
                elif len(items) >= 95:  # Allow 95% tolerance
                    self.log(f"‚úÖ EXCELLENT: V4 Enhanced scraper fetched {len(items)}/100 results (95%+ success)")
                    self.test_results["datasets"]["passed"] += 1
                elif len(items) >= 90:  # Allow 90% tolerance
                    self.log(f"‚úÖ GOOD: V4 Enhanced scraper fetched {len(items)}/100 results (90%+ success)")
                    self.test_results["datasets"]["passed"] += 1
                else:
                    self.log(f"‚ùå POOR COMPLETENESS: V4 Enhanced scraper only fetched {len(items)}/100 results (<90%)")
                    self.test_results["datasets"]["failed"] += 1
                    self.test_results["datasets"]["errors"].append(f"Poor completeness: only {len(items)}/100 results")
                
                # Verify no significant data loss (the original V4 issue was fixed)
                if len(items) >= 90:  # At least 90% of requested results
                    self.log("‚úÖ COMPLETENESS FIX VERIFIED: No significant data loss detected")
                    self.test_results["datasets"]["passed"] += 1
                else:
                    self.log("‚ùå COMPLETENESS ISSUE: Significant data loss detected (V4 Enhanced may have issues)")
                    self.test_results["datasets"]["failed"] += 1
                    self.test_results["datasets"]["errors"].append("Significant data loss detected in V4 Enhanced")
            else:
                self.log("‚ùå Completeness dataset items response is not a list")
                self.test_results["datasets"]["failed"] += 1
                self.test_results["datasets"]["errors"].append("Completeness dataset items response is not a list")
        else:
            self.log(f"‚ùå Get completeness dataset items failed: {response.status_code if response else 'No response'}")
            self.test_results["datasets"]["failed"] += 1
            self.test_results["datasets"]["errors"].append("Get completeness dataset items failed")

    def test_enhanced_global_chat_system(self):
        """Test Enhanced Global Chat System with Function Calling & Data Access"""
        self.log("=== Testing Enhanced Global Chat System with Function Calling ===")
        
        # Add global_chat to test results if not exists
        if "global_chat" not in self.test_results:
            self.test_results["global_chat"] = {"passed": 0, "failed": 0, "errors": []}
        
        # 1. Test Function Calling & Data Access
        self.log("--- Testing Function Calling & Data Access ---")
        
        # Test GET /api/chat/global/history - verify history loading
        self.log("Testing GET /api/chat/global/history...")
        response = self.make_request("GET", "/chat/global/history")
        if response and response.status_code == 200:
            history_data = response.json()
            if "history" in history_data and isinstance(history_data["history"], list):
                self.log(f"‚úÖ Chat history endpoint working (found {len(history_data['history'])} messages)")
                self.test_results["global_chat"]["passed"] += 1
            else:
                self.log("‚ùå Chat history response missing 'history' field or not a list")
                self.test_results["global_chat"]["failed"] += 1
                self.test_results["global_chat"]["errors"].append("Chat history response invalid structure")
        else:
            self.log(f"‚ùå Get chat history failed: {response.status_code if response else 'No response'}")
            self.test_results["global_chat"]["failed"] += 1
            self.test_results["global_chat"]["errors"].append("Get chat history failed")
        
        # Test data access questions that should trigger function calls
        data_access_questions = [
            {
                "question": "How many runs do I have?",
                "expected_function": "get_user_stats",
                "expected_terms": ["runs", "total", "statistics", "stats"]
            },
            {
                "question": "Show me my recent runs",
                "expected_function": "list_recent_runs", 
                "expected_terms": ["recent", "runs", "status", "scraping"]
            },
            {
                "question": "What scrapers are available?",
                "expected_function": "get_actors",
                "expected_terms": ["scrapers", "actors", "available", "google maps"]
            }
        ]
        
        for test_case in data_access_questions:
            self.log(f"Testing data access: '{test_case['question']}'")
            
            chat_request = {"message": test_case["question"]}
            response = self.make_request("POST", "/chat/global", chat_request)
            
            if response and response.status_code == 200:
                chat_response = response.json()
                if "response" in chat_response and "timestamp" in chat_response:
                    ai_response = chat_response["response"]
                    if len(ai_response) > 50:
                        self.log(f"‚úÖ Response received (length: {len(ai_response)} chars)")
                        self.test_results["global_chat"]["passed"] += 1
                        
                        # Check if response contains real data (not made up)
                        response_lower = ai_response.lower()
                        has_relevant_terms = any(term in response_lower for term in test_case["expected_terms"])
                        
                        if has_relevant_terms:
                            self.log(f"‚úÖ Response contains relevant data for {test_case['expected_function']}")
                            self.test_results["global_chat"]["passed"] += 1
                        else:
                            self.log(f"‚ö†Ô∏è Response may not contain expected data for {test_case['expected_function']}")
                        
                        # Check for numbers/data that suggest real database queries
                        import re
                        has_numbers = bool(re.search(r'\b\d+\b', ai_response))
                        if has_numbers:
                            self.log("‚úÖ Response contains numerical data (suggests real database access)")
                            self.test_results["global_chat"]["passed"] += 1
                        else:
                            self.log("‚ö†Ô∏è Response lacks numerical data")
                    else:
                        self.log("‚ùå Response too short")
                        self.test_results["global_chat"]["failed"] += 1
                        self.test_results["global_chat"]["errors"].append(f"Short response for: {test_case['question']}")
                else:
                    self.log("‚ùå Response missing required fields")
                    self.test_results["global_chat"]["failed"] += 1
                    self.test_results["global_chat"]["errors"].append(f"Invalid response for: {test_case['question']}")
            else:
                self.log(f"‚ùå Request failed: {response.status_code if response else 'No response'}")
                self.test_results["global_chat"]["failed"] += 1
                self.test_results["global_chat"]["errors"].append(f"Request failed for: {test_case['question']}")
        
        # 2. Test Natural Language Run Creation
        self.log("--- Testing Natural Language Run Creation ---")
        
        natural_language_tests = [
            {
                "request": "run google maps scraper for Hotels in New York with max 50 results",
                "expected_terms": ["hotels", "new york", "50", "scraping", "run"],
                "expected_location": "new york",
                "expected_search": "hotels",
                "expected_max": 50
            },
            {
                "request": "scrape restaurants in San Francisco",
                "expected_terms": ["restaurants", "san francisco", "scraping"],
                "expected_location": "san francisco", 
                "expected_search": "restaurants"
            }
        ]
        
        created_run_ids = []
        
        for test_case in natural_language_tests:
            self.log(f"Testing natural language run creation: '{test_case['request']}'")
            
            chat_request = {"message": test_case["request"]}
            response = self.make_request("POST", "/chat/global", chat_request)
            
            if response and response.status_code == 200:
                chat_response = response.json()
                ai_response = chat_response.get("response", "")
                
                # Check for run creation indicators (more flexible)
                run_created = any(phrase in ai_response.lower() for phrase in [
                    "run created successfully", "scraping run created", "successfully created",
                    "run has been", "run id:"
                ])
                
                if run_created:
                    self.log("‚úÖ Run creation confirmed in response")
                    self.test_results["global_chat"]["passed"] += 1
                    
                    # Extract run ID if present (more flexible patterns)
                    import re
                    run_id_patterns = [
                        r'Run ID:?\s*([a-f0-9\-]+)',
                        r'\*\*Run ID:\*\*\s*([a-f0-9\-]+)',
                        r'- \*\*Run ID:\*\*\s*([a-f0-9\-]+)'
                    ]
                    
                    run_id_match = None
                    for pattern in run_id_patterns:
                        run_id_match = re.search(pattern, ai_response, re.IGNORECASE)
                        if run_id_match:
                            break
                    if run_id_match:
                        run_id = run_id_match.group(1)
                        created_run_ids.append(run_id)
                        self.log(f"‚úÖ Run ID extracted: {run_id}")
                        self.test_results["global_chat"]["passed"] += 1
                        
                        # Verify run exists in database
                        time.sleep(2)  # Allow time for database write
                        run_response = self.make_request("GET", f"/runs/{run_id}")
                        if run_response and run_response.status_code == 200:
                            run_data = run_response.json()
                            if run_data.get("status") == "queued":
                                self.log("‚úÖ Run created in database with 'queued' status")
                                self.test_results["global_chat"]["passed"] += 1
                            else:
                                self.log(f"‚ö†Ô∏è Run status is '{run_data.get('status')}', expected 'queued'")
                            
                            # Verify parsing of parameters
                            input_data = run_data.get("input_data", {})
                            search_terms = input_data.get("search_terms", [])
                            location = input_data.get("location", "")
                            max_results = input_data.get("max_results", 0)
                            
                            if test_case["expected_search"].lower() in str(search_terms).lower():
                                self.log(f"‚úÖ Search terms parsed correctly: {search_terms}")
                                self.test_results["global_chat"]["passed"] += 1
                            else:
                                self.log(f"‚ùå Search terms not parsed correctly: {search_terms}")
                                self.test_results["global_chat"]["failed"] += 1
                                self.test_results["global_chat"]["errors"].append(f"Search terms parsing failed for: {test_case['request']}")
                            
                            if test_case["expected_location"].lower() in location.lower():
                                self.log(f"‚úÖ Location parsed correctly: {location}")
                                self.test_results["global_chat"]["passed"] += 1
                            else:
                                self.log(f"‚ùå Location not parsed correctly: {location}")
                                self.test_results["global_chat"]["failed"] += 1
                                self.test_results["global_chat"]["errors"].append(f"Location parsing failed for: {test_case['request']}")
                            
                            if "expected_max" in test_case and max_results == test_case["expected_max"]:
                                self.log(f"‚úÖ Max results parsed correctly: {max_results}")
                                self.test_results["global_chat"]["passed"] += 1
                            elif "expected_max" in test_case:
                                self.log(f"‚ùå Max results not parsed correctly: {max_results}, expected: {test_case['expected_max']}")
                                self.test_results["global_chat"]["failed"] += 1
                                self.test_results["global_chat"]["errors"].append(f"Max results parsing failed for: {test_case['request']}")
                        else:
                            self.log("‚ùå Created run not found in database")
                            self.test_results["global_chat"]["failed"] += 1
                            self.test_results["global_chat"]["errors"].append(f"Run not found in database for: {test_case['request']}")
                    else:
                        self.log("‚ö†Ô∏è Run ID not found in response")
                else:
                    self.log("‚ùå Run creation not confirmed in response")
                    self.test_results["global_chat"]["failed"] += 1
                    self.test_results["global_chat"]["errors"].append(f"Run creation failed for: {test_case['request']}")
            else:
                self.log(f"‚ùå Natural language request failed: {response.status_code if response else 'No response'}")
                self.test_results["global_chat"]["failed"] += 1
                self.test_results["global_chat"]["errors"].append(f"Request failed for: {test_case['request']}")
        
        # 3. Test Conversation Persistence
        self.log("--- Testing Conversation Persistence ---")
        
        # Clear history first
        self.log("Clearing chat history...")
        clear_response = self.make_request("DELETE", "/chat/global/history")
        if clear_response and clear_response.status_code == 200:
            self.log("‚úÖ Chat history cleared")
            self.test_results["global_chat"]["passed"] += 1
        else:
            self.log("‚ùå Failed to clear chat history")
            self.test_results["global_chat"]["failed"] += 1
            self.test_results["global_chat"]["errors"].append("Failed to clear chat history")
        
        # Send multiple messages in sequence
        conversation_messages = [
            "What is Scrapi?",
            "How do I get started?",
            "Can you tell me more about the previous topic?"
        ]
        
        for i, message in enumerate(conversation_messages):
            self.log(f"Sending message {i+1}: '{message}'")
            
            chat_request = {"message": message}
            response = self.make_request("POST", "/chat/global", chat_request)
            
            if response and response.status_code == 200:
                self.log(f"‚úÖ Message {i+1} sent successfully")
                self.test_results["global_chat"]["passed"] += 1
            else:
                self.log(f"‚ùå Message {i+1} failed")
                self.test_results["global_chat"]["failed"] += 1
                self.test_results["global_chat"]["errors"].append(f"Message {i+1} failed")
        
        # Verify all messages saved in global_chat_history collection
        time.sleep(2)  # Allow time for database writes
        history_response = self.make_request("GET", "/chat/global/history")
        if history_response and history_response.status_code == 200:
            history_data = history_response.json()
            history = history_data.get("history", [])
            
            # Should have user + assistant messages for each conversation
            expected_min_messages = len(conversation_messages) * 2  # user + assistant for each
            if len(history) >= expected_min_messages:
                self.log(f"‚úÖ Conversation persistence working ({len(history)} messages saved)")
                self.test_results["global_chat"]["passed"] += 1
            else:
                self.log(f"‚ùå Not all messages saved ({len(history)} found, expected at least {expected_min_messages})")
                self.test_results["global_chat"]["failed"] += 1
                self.test_results["global_chat"]["errors"].append("Conversation persistence failed")
        
        # 4. Test Response Quality
        self.log("--- Testing Response Quality ---")
        
        quality_test_message = "Tell me about Scrapi features"
        chat_request = {"message": quality_test_message}
        response = self.make_request("POST", "/chat/global", chat_request)
        
        if response and response.status_code == 200:
            chat_response = response.json()
            ai_response = chat_response.get("response", "")
            
            # Check for proper markdown formatting
            has_bold = "**" in ai_response
            has_lists = any(marker in ai_response for marker in ["- ", "* ", "1. ", "2. "])
            
            if has_bold or has_lists:
                self.log("‚úÖ Response uses markdown formatting")
                self.test_results["global_chat"]["passed"] += 1
            else:
                self.log("‚ö†Ô∏è Response lacks markdown formatting")
            
            # Check for raw markdown symbols (should be processed)
            has_raw_markdown = "###" in ai_response or ai_response.count("**") % 2 != 0
            if not has_raw_markdown:
                self.log("‚úÖ No raw markdown symbols in response")
                self.test_results["global_chat"]["passed"] += 1
            else:
                self.log("‚ùå Response contains raw markdown symbols")
                self.test_results["global_chat"]["failed"] += 1
                self.test_results["global_chat"]["errors"].append("Response contains raw markdown symbols")
            
            # Check if response is contextual and helpful
            contextual_terms = ["scrapi", "scraping", "platform", "features", "actors", "runs"]
            is_contextual = any(term in ai_response.lower() for term in contextual_terms)
            
            if is_contextual:
                self.log("‚úÖ Response is contextual and helpful")
                self.test_results["global_chat"]["passed"] += 1
            else:
                self.log("‚ùå Response may not be contextual")
                self.test_results["global_chat"]["failed"] += 1
                self.test_results["global_chat"]["errors"].append("Response not contextual")
        
        # 5. Test Edge Cases
        self.log("--- Testing Edge Cases ---")
        
        # Test invalid run creation request
        self.log("Testing invalid run creation request...")
        invalid_request = {"message": "run nonexistent scraper for nothing"}
        response = self.make_request("POST", "/chat/global", invalid_request)
        
        if response and response.status_code == 200:
            chat_response = response.json()
            ai_response = chat_response.get("response", "")
            
            if "error" in ai_response.lower() or "not found" in ai_response.lower() or "invalid" in ai_response.lower():
                self.log("‚úÖ Invalid request handled gracefully")
                self.test_results["global_chat"]["passed"] += 1
            else:
                self.log("‚ö†Ô∏è Invalid request may not be handled properly")
        
        # Test stop_run and delete_run functions with valid run_id (if we have created runs)
        if created_run_ids:
            test_run_id = created_run_ids[0]
            
            # Test stop_run
            self.log(f"Testing stop_run function with run_id: {test_run_id}")
            stop_request = {"message": f"stop run {test_run_id}"}
            response = self.make_request("POST", "/chat/global", stop_request)
            
            if response and response.status_code == 200:
                chat_response = response.json()
                ai_response = chat_response.get("response", "")
                
                if "stopped" in ai_response.lower() or "aborted" in ai_response.lower():
                    self.log("‚úÖ Stop run function working")
                    self.test_results["global_chat"]["passed"] += 1
                else:
                    self.log("‚ö†Ô∏è Stop run function may not be working")
            
            # Test delete_run
            self.log(f"Testing delete_run function with run_id: {test_run_id}")
            delete_request = {"message": f"delete run {test_run_id}"}
            response = self.make_request("POST", "/chat/global", delete_request)
            
            if response and response.status_code == 200:
                chat_response = response.json()
                ai_response = chat_response.get("response", "")
                
                if "deleted" in ai_response.lower():
                    self.log("‚úÖ Delete run function working")
                    self.test_results["global_chat"]["passed"] += 1
                else:
                    self.log("‚ö†Ô∏è Delete run function may not be working")
        
        # Test error handling for missing data
        self.log("Testing error handling...")
        error_request = {"message": "show me run with id nonexistent-id"}
        response = self.make_request("POST", "/chat/global", error_request)
        
        if response and response.status_code == 200:
            chat_response = response.json()
            ai_response = chat_response.get("response", "")
            
            if "not found" in ai_response.lower() or "error" in ai_response.lower():
                self.log("‚úÖ Error handling working for missing data")
                self.test_results["global_chat"]["passed"] += 1
            else:
                self.log("‚ö†Ô∏è Error handling may not be working properly")
        
        self.log("Enhanced Global Chat System testing completed")
            
    def run_all_tests(self):
        """Run all test suites"""
        self.log("Starting Scrapi Backend API Tests...")
        self.log(f"Backend URL: {self.base_url}")
        
        # Test authentication first
        if not self.test_auth_flow():
            self.log("‚ùå Authentication failed - stopping tests")
            return
            
        # Test initial state verification
        self.test_initial_state_verification()
        
        # Test scraper creation system as requested in review
        self.test_scraper_creation_system()
        
        # Test end-to-end workflow with real data
        self.test_end_to_end_scraper_workflow()
        
        # Skip other tests for focused scraper creation testing
        # self.test_google_maps_scraper_v4_enhanced_super_fast()
        # self.test_data_isolation_fix()
        # self.test_actors_management()
        # self.test_proxy_system()
        # self.test_enhanced_scraping_v3()
        # self.test_enhanced_dataset_v3()
        # self.test_ai_chat_system()
        # self.test_enhanced_global_chat_system()
        
        # Print summary
        self.print_summary()
        
    def print_summary(self):
        """Print test results summary"""
        self.log("\n" + "="*60)
        self.log("TEST RESULTS SUMMARY")
        self.log("="*60)
        
        total_passed = 0
        total_failed = 0
        
        for category, results in self.test_results.items():
            passed = results["passed"]
            failed = results["failed"]
            total_passed += passed
            total_failed += failed
            
            status = "‚úÖ PASS" if failed == 0 else "‚ùå FAIL"
            self.log(f"{category.upper()}: {status} ({passed} passed, {failed} failed)")
            
            if results["errors"]:
                for error in results["errors"]:
                    self.log(f"  - {error}")
                    
        self.log("-" * 60)
        self.log(f"OVERALL: {total_passed} passed, {total_failed} failed")
        
        if total_failed == 0:
            self.log("üéâ ALL TESTS PASSED!")
        else:
            self.log(f"‚ö†Ô∏è  {total_failed} TESTS FAILED")
            
        return total_failed == 0

if __name__ == "__main__":
    tester = ScrapiAPITester()
    success = tester.run_all_tests()
    sys.exit(0 if success else 1)